{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0978a56-7776-4003-af49-b3fafee3154c",
   "metadata": {},
   "source": [
    "# Fuzzy Matching\n",
    "\n",
    "This notebook compares \"official\" organization names from two databases: the Orbis company name database, and the GDelt knowledge graph of news articles. \"Official\" Orbis company names are reconciled with alternative organization identifiers, taking in an Orbis company name and returning all alternative names mentioned in the GDelt dataset, such as alternative spellings and naming variations that refer to the same organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bec8f99-d710-482a-8b48-a6fe188b7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc18e645-1154-4b64-9095-6bad72966907",
   "metadata": {},
   "source": [
    "## Execution settings\n",
    "\n",
    "Set the input sources, number of Orbis company names, and the number of GDelt press articles to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b7cb3f1-66bd-42f9-86aa-4c1912bed44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "ORBIS_INPUT = './input/orbis_test_small.xlsx' # Excel\n",
    "GDELT_INPUT = './input/gdelt_test.csv' # CSV\n",
    "\n",
    "# Number of Orbis records.\n",
    "NUM_ROWS_ORBIS = 3000\n",
    "\n",
    "# Range/number of GDelt records.\n",
    "NUM_ROWS_GDELT_START = 0 \n",
    "NUM_ROWS_GDELT_END = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f74c4e-9fb8-4863-bf51-ac5d4935f655",
   "metadata": {},
   "source": [
    "#### Orbis test datasets:\n",
    "- List of Orbis companies in Sierra Leone: orbis_sierra_leone.xlsx\n",
    "- Large list of Orbis companies in Sierra Leone: orbis_test_large.xlsx\n",
    "- Small list of Orbis companies for testing: orbis_test_small.xlsx\n",
    "\n",
    "#### GDelt test datasets:\n",
    "- List of GDelt articless in Sierra Leone in 2020: gdelt_test.csv\n",
    "\n",
    "#### Test inputs:\n",
    "- https://drive.google.com/drive/folders/15QiHluI3dIIWPW6VLMXcxmeM2KuYD0JL?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f939c8-8d9a-4e0b-bfd5-d76f1fc42187",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9382be3b-b1f2-4dd6-8bfd-b67b7317206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f4ae4-2b1e-40e3-b84c-372826affb09",
   "metadata": {},
   "source": [
    "### Orbis Data\n",
    "\n",
    "Load a CSV file of \"official\" company names, which can be obtained by querying the Orbis company database. You can limit search results by adding filter criteria, such as the country of registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f0999a-8c8f-4239-9f3f-f558a509de90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl\n",
    "\n",
    "indata_orbis = pd.read_excel(ORBIS_INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edf1aa76-d5cf-4319-abd6-751e09a2c1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indata_orbis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6067cb3a-4535-49ba-a517-1bb9891ead50",
   "metadata": {},
   "outputs": [],
   "source": [
    "indata_orbis = indata_orbis[:NUM_ROWS_ORBIS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28d3e1b8-2bba-4fef-9195-d6c0fa8336dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "indata_orbis = indata_orbis[['Company name Latin alphabet']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50decdbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  international business machines corp\n",
       "1                                            pfizer inc\n",
       "2                                 eli lilly and company\n",
       "3                       south african airways (pty) ltd\n",
       "4                                  ryanair holdings plc\n",
       "5                                      associated press\n",
       "6     u.s. international development finance corpora...\n",
       "7                       bill & melinda gates foundation\n",
       "8                                      world bank group\n",
       "9                 european union trading and agency sae\n",
       "10                                  royal navy reserves\n",
       "11                                 africell holding sal\n",
       "12                                   clinton foundation\n",
       "13    united nations international fund for agricult...\n",
       "14                                alliance news limited\n",
       "15                            world health organization\n",
       "Name: Company name Latin alphabet, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " indata_orbis['Company name Latin alphabet'].apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5393e13e-092b-41dc-bf80-aad25f48e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indata_orbis['name_original'] = indata_orbis['Company name Latin alphabet']\n",
    "indata_orbis['name'] = pd.DataFrame(indata_orbis['Company name Latin alphabet'].apply(str.lower))\n",
    "outdata_orbis = indata_orbis[['name_original', 'name']]\n",
    "outdata_orbis.sort_values(by='name', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050a85fd-2b5e-45b7-a6af-6ad7e79c8259",
   "metadata": {},
   "source": [
    "#### Clean company names\n",
    "\n",
    "We need to clean company names to get rid of odd artifacts and other wrinkles. Remove anything in parenthesis, all punctuation, and any extra whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0af279a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company name Latin alphabet</th>\n",
       "      <th>name_original</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INTERNATIONAL BUSINESS MACHINES CORP</td>\n",
       "      <td>INTERNATIONAL BUSINESS MACHINES CORP</td>\n",
       "      <td>international business machines corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PFIZER INC</td>\n",
       "      <td>PFIZER INC</td>\n",
       "      <td>pfizer inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ELI LILLY AND COMPANY</td>\n",
       "      <td>ELI LILLY AND COMPANY</td>\n",
       "      <td>eli lilly and company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SOUTH AFRICAN AIRWAYS (PTY) LTD</td>\n",
       "      <td>SOUTH AFRICAN AIRWAYS (PTY) LTD</td>\n",
       "      <td>south african airways (pty) ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RYANAIR HOLDINGS PLC</td>\n",
       "      <td>RYANAIR HOLDINGS PLC</td>\n",
       "      <td>ryanair holdings plc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Company name Latin alphabet                         name_original  \\\n",
       "0  INTERNATIONAL BUSINESS MACHINES CORP  INTERNATIONAL BUSINESS MACHINES CORP   \n",
       "1                            PFIZER INC                            PFIZER INC   \n",
       "2                 ELI LILLY AND COMPANY                 ELI LILLY AND COMPANY   \n",
       "3       SOUTH AFRICAN AIRWAYS (PTY) LTD       SOUTH AFRICAN AIRWAYS (PTY) LTD   \n",
       "4                  RYANAIR HOLDINGS PLC                  RYANAIR HOLDINGS PLC   \n",
       "\n",
       "                                   name  \n",
       "0  international business machines corp  \n",
       "1                            pfizer inc  \n",
       "2                 eli lilly and company  \n",
       "3       south african airways (pty) ltd  \n",
       "4                  ryanair holdings plc  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indata_orbis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ec41494-2e2e-4805-891d-0319733944ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Helper methods for name cleaning.\n",
    "\n",
    "def remove_parenthesis(name):\n",
    "    import regex as re\n",
    "    return re.sub(r'\\(.*\\)', '', name)\n",
    "\n",
    "def remove_punctuation(name):\n",
    "    return name.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_double_space(name):\n",
    "    name = ' '.join(name.split())\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f920e-2e5f-40b1-acdb-1aac9e367e96",
   "metadata": {},
   "source": [
    "The \"cleanco\" package removes company suffixes such as \"inc.\" and \"limited\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44d9fa0f-506c-4010-90f3-d516f4972fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cleanco in c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages (2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install cleanco\n",
    "from cleanco import basename\n",
    "\n",
    "outdata_orbis['name_clean'] = outdata_orbis['name'].apply(str.strip)\n",
    "outdata_orbis['name_clean'] = outdata_orbis['name_clean'].apply(remove_parenthesis)\n",
    "outdata_orbis['name_clean'] = outdata_orbis['name_clean'].apply(remove_punctuation)\n",
    "outdata_orbis['name_clean'] = outdata_orbis['name_clean'].apply(basename)\n",
    "outdata_orbis['name_clean'] = outdata_orbis['name_clean'].apply(basename) # run basename() twice because of multiple suffixes\n",
    "outdata_orbis['name_clean'] = outdata_orbis['name_clean'].apply(remove_double_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "197f24f0-ae00-427b-8276-158cd66b229a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_original</th>\n",
       "      <th>name</th>\n",
       "      <th>name_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ALLIANCE NEWS LIMITED</td>\n",
       "      <td>alliance news limited</td>\n",
       "      <td>alliance news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EUROPEAN UNION TRADING AND AGENCY SAE</td>\n",
       "      <td>european union trading and agency sae</td>\n",
       "      <td>european union trading and agency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SOUTH AFRICAN AIRWAYS (PTY) LTD</td>\n",
       "      <td>south african airways (pty) ltd</td>\n",
       "      <td>south african airways</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PFIZER INC</td>\n",
       "      <td>pfizer inc</td>\n",
       "      <td>pfizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ELI LILLY AND COMPANY</td>\n",
       "      <td>eli lilly and company</td>\n",
       "      <td>eli lilly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name_original  \\\n",
       "14                  ALLIANCE NEWS LIMITED   \n",
       "9   EUROPEAN UNION TRADING AND AGENCY SAE   \n",
       "3         SOUTH AFRICAN AIRWAYS (PTY) LTD   \n",
       "1                              PFIZER INC   \n",
       "2                   ELI LILLY AND COMPANY   \n",
       "\n",
       "                                     name                         name_clean  \n",
       "14                  alliance news limited                      alliance news  \n",
       "9   european union trading and agency sae  european union trading and agency  \n",
       "3         south african airways (pty) ltd              south african airways  \n",
       "1                              pfizer inc                             pfizer  \n",
       "2                   eli lilly and company                          eli lilly  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdata_orbis.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61a26bab-e435-46a8-a967-9a3c5673240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdata_orbis.to_csv('./output/orbis_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684cc895-6c54-4009-95d8-f2ce1a97a619",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GDelt Data\n",
    "\n",
    "Load a CSV file of press mentions, which can be obtained by querying the GDelt global knowledge database. You can limit search results by adding filter criteria, such as press mentions by country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3600b59e-aa14-43d0-8f8b-fb3f802d0ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "indata_gdelt = pd.read_csv(GDELT_INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3f83cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gkgrecordid</th>\n",
       "      <th>date</th>\n",
       "      <th>sourcecollectionidentifier</th>\n",
       "      <th>sourcecommonname</th>\n",
       "      <th>documentidentifier</th>\n",
       "      <th>counts</th>\n",
       "      <th>locations</th>\n",
       "      <th>organizations</th>\n",
       "      <th>themes</th>\n",
       "      <th>persons</th>\n",
       "      <th>tone</th>\n",
       "      <th>dates</th>\n",
       "      <th>amounts</th>\n",
       "      <th>translationinfo</th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200316171500-563</td>\n",
       "      <td>2020-03-16 17:15:00.000</td>\n",
       "      <td>1</td>\n",
       "      <td>570news.com</td>\n",
       "      <td>https://www.570news.com/2020/03/16/new-africa-...</td>\n",
       "      <td>[{count_type=KILL, count=11, object_type=, loc...</td>\n",
       "      <td>[{location_type=4, location_fullname=Monrovia,...</td>\n",
       "      <td>[{organization=Associated Press, character_off...</td>\n",
       "      <td>[{theme=UNGP_EDUCATION, character_offset=4791}...</td>\n",
       "      <td>[{person=Shakira Choonara, character_offset=40...</td>\n",
       "      <td>{tone=-3.86052303860523, positive_score=3.1133...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{amount=30, object=of Africa 54 countries, ch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SL</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200316171500-973</td>\n",
       "      <td>2020-03-16 17:15:00.000</td>\n",
       "      <td>1</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>https://news.yahoo.com/africell-holding-comple...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{location_type=1, location_fullname=Sierra Le...</td>\n",
       "      <td>[{organization=Africell Holding, character_off...</td>\n",
       "      <td>[{theme=GENERAL_GOVERNMENT, character_offset=1...</td>\n",
       "      <td>[{person=Ziad Dalloul, character_offset=817}, ...</td>\n",
       "      <td>{tone=4.67445742904841, positive_score=5.34223...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{amount=12000000, object=customers, character...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SL</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200316171500-1417</td>\n",
       "      <td>2020-03-16 17:15:00.000</td>\n",
       "      <td>1</td>\n",
       "      <td>stuff.co.nz</td>\n",
       "      <td>https://www.stuff.co.nz/timaru-herald/news/120...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{location_type=1, location_fullname=Sierra Le...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{theme=TAX_WORLDFISH_BANJOS, character_offset...</td>\n",
       "      <td>[{person=John Trotter, character_offset=2650},...</td>\n",
       "      <td>{tone=1.6, positive_score=2.2, negative_score=...</td>\n",
       "      <td>[{date_resolution=1, month=0, day=0, year=1963...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SL</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200316171500-1483</td>\n",
       "      <td>2020-03-16 17:15:00.000</td>\n",
       "      <td>1</td>\n",
       "      <td>bmj.com</td>\n",
       "      <td>https://www.bmj.com/content/368/bmj.m481/rr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{location_type=1, location_fullname=Sierra Le...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{theme=GENERAL_GOVERNMENT, character_offset=3...</td>\n",
       "      <td>[{person=Jenny Gibson, character_offset=63}]</td>\n",
       "      <td>{tone=1.36054421768708, positive_score=3.40136...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SL</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200316171500-2485</td>\n",
       "      <td>2020-03-16 17:15:00.000</td>\n",
       "      <td>1</td>\n",
       "      <td>venturesafrica.com</td>\n",
       "      <td>http://venturesafrica.com/apostories/25-police...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{location_type=1, location_fullname=Sierra Le...</td>\n",
       "      <td>[{organization=Police Contributing Countries P...</td>\n",
       "      <td>[{theme=TAX_FNCACT_ASSISTANT, character_offset...</td>\n",
       "      <td>[{person=Rex Dundun, character_offset=1319}, {...</td>\n",
       "      <td>{tone=1.04712041884817, positive_score=2.35602...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{amount=25, object=newly deployed Individual ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SL</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           gkgrecordid                     date  sourcecollectionidentifier  \\\n",
       "0   20200316171500-563  2020-03-16 17:15:00.000                           1   \n",
       "1   20200316171500-973  2020-03-16 17:15:00.000                           1   \n",
       "2  20200316171500-1417  2020-03-16 17:15:00.000                           1   \n",
       "3  20200316171500-1483  2020-03-16 17:15:00.000                           1   \n",
       "4  20200316171500-2485  2020-03-16 17:15:00.000                           1   \n",
       "\n",
       "     sourcecommonname                                 documentidentifier  \\\n",
       "0         570news.com  https://www.570news.com/2020/03/16/new-africa-...   \n",
       "1           yahoo.com  https://news.yahoo.com/africell-holding-comple...   \n",
       "2         stuff.co.nz  https://www.stuff.co.nz/timaru-herald/news/120...   \n",
       "3             bmj.com        https://www.bmj.com/content/368/bmj.m481/rr   \n",
       "4  venturesafrica.com  http://venturesafrica.com/apostories/25-police...   \n",
       "\n",
       "                                              counts  \\\n",
       "0  [{count_type=KILL, count=11, object_type=, loc...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                           locations  \\\n",
       "0  [{location_type=4, location_fullname=Monrovia,...   \n",
       "1  [{location_type=1, location_fullname=Sierra Le...   \n",
       "2  [{location_type=1, location_fullname=Sierra Le...   \n",
       "3  [{location_type=1, location_fullname=Sierra Le...   \n",
       "4  [{location_type=1, location_fullname=Sierra Le...   \n",
       "\n",
       "                                       organizations  \\\n",
       "0  [{organization=Associated Press, character_off...   \n",
       "1  [{organization=Africell Holding, character_off...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  [{organization=Police Contributing Countries P...   \n",
       "\n",
       "                                              themes  \\\n",
       "0  [{theme=UNGP_EDUCATION, character_offset=4791}...   \n",
       "1  [{theme=GENERAL_GOVERNMENT, character_offset=1...   \n",
       "2  [{theme=TAX_WORLDFISH_BANJOS, character_offset...   \n",
       "3  [{theme=GENERAL_GOVERNMENT, character_offset=3...   \n",
       "4  [{theme=TAX_FNCACT_ASSISTANT, character_offset...   \n",
       "\n",
       "                                             persons  \\\n",
       "0  [{person=Shakira Choonara, character_offset=40...   \n",
       "1  [{person=Ziad Dalloul, character_offset=817}, ...   \n",
       "2  [{person=John Trotter, character_offset=2650},...   \n",
       "3       [{person=Jenny Gibson, character_offset=63}]   \n",
       "4  [{person=Rex Dundun, character_offset=1319}, {...   \n",
       "\n",
       "                                                tone  \\\n",
       "0  {tone=-3.86052303860523, positive_score=3.1133...   \n",
       "1  {tone=4.67445742904841, positive_score=5.34223...   \n",
       "2  {tone=1.6, positive_score=2.2, negative_score=...   \n",
       "3  {tone=1.36054421768708, positive_score=3.40136...   \n",
       "4  {tone=1.04712041884817, positive_score=2.35602...   \n",
       "\n",
       "                                               dates  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  [{date_resolution=1, month=0, day=0, year=1963...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             amounts  translationinfo country  \\\n",
       "0  [{amount=30, object=of Africa 54 countries, ch...              NaN      SL   \n",
       "1  [{amount=12000000, object=customers, character...              NaN      SL   \n",
       "2                                                NaN              NaN      SL   \n",
       "3                                                NaN              NaN      SL   \n",
       "4  [{amount=25, object=newly deployed Individual ...              NaN      SL   \n",
       "\n",
       "   year  \n",
       "0  2020  \n",
       "1  2020  \n",
       "2  2020  \n",
       "3  2020  \n",
       "4  2020  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indata_gdelt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c35c79f-ae8b-4990-8964-3dedeef27b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indata_gdelt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49c5790a-ec50-410b-9b7f-5a6bfef248be",
   "metadata": {},
   "outputs": [],
   "source": [
    "indata_gdelt = indata_gdelt[['organizations']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02c2de80-4b8b-44f2-9eae-d6c8641248d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs_unextracted_gdelt = []\n",
    "\n",
    "for index, row in indata_gdelt.iterrows():\n",
    "    # row is a single-item list with a string surrounded\n",
    "    # by curly braces. Extract the single item and remove\n",
    "    # the surrounding curly braces.\n",
    "    orgs_unextracted_gdelt.append(row[0][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ab66fc7-19ec-4f8e-878f-c524e06e6874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "orgs_extracted_gdelt = []\n",
    "\n",
    "# The rows are json-like formatted strings that contain non-quoted\n",
    "# information which includes company names, each of which can be extracted \n",
    "# via regex and be treated as a subrow.\n",
    "for row in orgs_unextracted_gdelt:\n",
    "    row = row.split('},')\n",
    "    for subrow in row:\n",
    "        match = re.findall(r'(?:n=)(.*)(?:,)', subrow)\n",
    "        orgs_extracted_gdelt.append(match[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94b92629-709e-4bd9-9f58-417fde156b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs_extracted_gdelt = pd.DataFrame(orgs_extracted_gdelt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccb4757f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Associated Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Associated Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Associated Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Africa National Institute For Communicable Dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                                   Associated Press\n",
       "1                                   Associated Press\n",
       "2                                   Associated Press\n",
       "3  Africa National Institute For Communicable Dis...\n",
       "4                                      United States"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orgs_extracted_gdelt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38fc7c4b-78f3-42f8-8707-4cd715474b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdata_gdelt = pd.DataFrame(orgs_extracted_gdelt.value_counts())\n",
    "outdata_gdelt.rename(columns={0: 'freq_gdelt'}, inplace=True)\n",
    "outdata_gdelt.reset_index(inplace=True)\n",
    "outdata_gdelt.rename(columns={0: 'name_gdelt'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a84c545-f3ec-462b-b911-d201de206d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdata_gdelt = outdata_gdelt[NUM_ROWS_GDELT_START:NUM_ROWS_GDELT_END]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2abdb055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_gdelt</th>\n",
       "      <th>freq_gdelt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>39691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Foundation Trust</td>\n",
       "      <td>30845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ambulance Service</td>\n",
       "      <td>10249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United Nations</td>\n",
       "      <td>7801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>World Health Organization</td>\n",
       "      <td>6566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name_gdelt  freq_gdelt\n",
       "0              United States       39691\n",
       "1           Foundation Trust       30845\n",
       "2          Ambulance Service       10249\n",
       "3             United Nations        7801\n",
       "4  World Health Organization        6566"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdata_gdelt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c9c0ec2-3fc2-463d-96b2-5917a56825e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outdata_gdelt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e099dd00-db43-452a-8918-dfefc754cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdata_gdelt['name_original'] = outdata_gdelt['name_gdelt']\n",
    "outdata_gdelt['name_gdelt'] = pd.DataFrame(outdata_gdelt['name_gdelt'].apply(str.lower))\n",
    "outdata_gdelt['name_gdelt'] = outdata_gdelt['name_gdelt'].apply(str.strip)\n",
    "outdata_gdelt['name_gdelt'] = outdata_gdelt['name_gdelt'].apply(remove_parenthesis)\n",
    "outdata_gdelt['name_gdelt'] = outdata_gdelt['name_gdelt'].apply(remove_punctuation)\n",
    "outdata_gdelt['name_gdelt'] = outdata_gdelt['name_gdelt'].apply(basename)\n",
    "outdata_gdelt['name_gdelt'] = outdata_gdelt['name_gdelt'].apply(basename) # run basename() twice because of multiple suffixes\n",
    "outdata_gdelt['name_gdelt'] = outdata_gdelt['name_gdelt'].apply(remove_double_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a365cddb-cb7f-43e1-8ed0-3bea296191d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_gdelt</th>\n",
       "      <th>freq_gdelt</th>\n",
       "      <th>name_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>news agency</td>\n",
       "      <td>98</td>\n",
       "      <td>News Agency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>finland national bureau of investigation</td>\n",
       "      <td>39</td>\n",
       "      <td>Finland National Bureau Of Investigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>university of queensland centre for clinical r...</td>\n",
       "      <td>44</td>\n",
       "      <td>University Of Queensland Centre For Clinical R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>pa mike brown</td>\n",
       "      <td>673</td>\n",
       "      <td>Pa Mike Brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>kotota international airport</td>\n",
       "      <td>67</td>\n",
       "      <td>Kotota International Airport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            name_gdelt  freq_gdelt  \\\n",
       "386                                        news agency          98   \n",
       "895           finland national bureau of investigation          39   \n",
       "815  university of queensland centre for clinical r...          44   \n",
       "87                                       pa mike brown         673   \n",
       "561                       kotota international airport          67   \n",
       "\n",
       "                                         name_original  \n",
       "386                                        News Agency  \n",
       "895           Finland National Bureau Of Investigation  \n",
       "815  University Of Queensland Centre For Clinical R...  \n",
       "87                                       Pa Mike Brown  \n",
       "561                       Kotota International Airport  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdata_gdelt.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b972a3-e588-4ef5-9351-9fa420c4bf55",
   "metadata": {},
   "source": [
    "#### Acronyms\n",
    "\n",
    "Although it's possible to compare company acronyms, there's not enough information embedded in an acronym to confidently match to full company names. For example, \"US\" could refer to \"United States\" or \"United Steel\" or \"Universal Studios\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faa8c624-c244-477c-9344-ea5b8b50ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not account for \"company\" or \"inc\" at the end of the full name.\n",
    "# def create_acronym(name):\n",
    "#     words = name.split() \n",
    "#     output = ''\n",
    "#     for word in words: \n",
    "#         output += word[0] \n",
    "#     return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "639ac9d1-ea98-4d19-9cc8-cca6cc709994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not really used at the moment.\n",
    "# outdata_gdelt['acronym_gdelt'] = outdata_gdelt['name_gdelt'].apply(create_acronym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75a6d76b-1ab4-41bb-ad3f-2d3117aecdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdata_gdelt.to_csv('./output/gdelt_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b219d1-ca88-40c9-9323-33bac3b30047",
   "metadata": {},
   "source": [
    "# Scoring\n",
    "\n",
    "Several scoring methods are implemented below, including those from py_stringsimjoin, py_stringmatching, FuzzyWuzzy, and Jellyfish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "491ccc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module for data manipulation\n",
    "import pandas as pd\n",
    "# Import module for linear algebra\n",
    "import numpy as np\n",
    "# Import module for Fuzzy string matching\n",
    "from fuzzywuzzy import fuzz, process\n",
    "# Import module for regex\n",
    "import re\n",
    "# Import module for iteration\n",
    "import itertools\n",
    "# Import module for function development\n",
    "from typing import Union, List, Tuple\n",
    "# Import module for TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Import module for cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Import module for KNN\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccf5588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String matching - TF-IDF\n",
    "def build_vectorizer(\n",
    "    clean: pd.Series,\n",
    "    analyzer: str = 'char', \n",
    "    ngram_range: Tuple[int, int] = (1, 4), \n",
    "    n_neighbors: int = 1, \n",
    "    **kwargs\n",
    "    ) -> Tuple:\n",
    "    # Create vectorizer\n",
    "    vectorizer = TfidfVectorizer(analyzer = analyzer, ngram_range = ngram_range, **kwargs)\n",
    "    X = vectorizer.fit_transform(clean.values.astype('U'))\n",
    "\n",
    "    # Fit nearest neighbors corpus\n",
    "    nbrs = NearestNeighbors(n_neighbors = n_neighbors, metric = 'cosine').fit(X)\n",
    "    return vectorizer, nbrs\n",
    "\n",
    "# String matching - KNN\n",
    "def tfidf_nn(\n",
    "    messy, \n",
    "    clean, \n",
    "    n_neighbors = 1, \n",
    "    **kwargs\n",
    "    ):\n",
    "    # Fit clean data and transform messy data\n",
    "    vectorizer, nbrs = build_vectorizer(clean, n_neighbors = n_neighbors, **kwargs)\n",
    "    input_vec = vectorizer.transform(messy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a7eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cd7940a-12a8-47ca-a7ae-2267a7de0236",
   "metadata": {},
   "source": [
    "### py_stringsimjoin\n",
    "\n",
    "Given two tables A and B, this package provides commands to perform string similarity joins between two columns of these tables, such as A.name and B.name, or A.city and B.city.\n",
    "\n",
    "http://anhaidgroup.github.io/py_stringsimjoin/v0.1.x/overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2628807a-fea6-4285-8342-382a79c181b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py_stringsimjoin\n",
      "  Using cached py_stringsimjoin-0.3.2.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: joblib in c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages (from py_stringsimjoin) (1.1.0)\n",
      "Requirement already satisfied: pandas>=0.16.0 in c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages (from py_stringsimjoin) (1.3.2)\n",
      "Requirement already satisfied: PyPrind>=2.9.3 in c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages (from py_stringsimjoin) (2.11.3)\n",
      "Collecting py_stringmatching>=0.2.1\n",
      "  Using cached py_stringmatching-0.4.2.tar.gz (661 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages (from py_stringsimjoin) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages (from pandas>=0.16.0->py_stringsimjoin) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages (from pandas>=0.16.0->py_stringsimjoin) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages (from pandas>=0.16.0->py_stringsimjoin) (2.8.2)\n",
      "Building wheels for collected packages: py_stringsimjoin, py_stringmatching\n",
      "  Building wheel for py_stringsimjoin (setup.py): started\n",
      "  Building wheel for py_stringsimjoin (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for py_stringsimjoin\n",
      "  Building wheel for py_stringmatching (setup.py): started\n",
      "  Building wheel for py_stringmatching (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for py_stringmatching\n",
      "Failed to build py_stringsimjoin py_stringmatching\n",
      "Installing collected packages: py_stringmatching, py_stringsimjoin\n",
      "  Running setup.py install for py_stringmatching: started\n",
      "  Running setup.py install for py_stringmatching: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [126 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-3.8\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringsimjoin\n",
      "      copying py_stringsimjoin\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringsimjoin\\datasets\n",
      "      copying py_stringsimjoin\\datasets\\base.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\datasets\n",
      "      copying py_stringsimjoin\\datasets\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\datasets\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringsimjoin\\filter\n",
      "      copying py_stringsimjoin\\filter\\filter.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\filter\n",
      "      copying py_stringsimjoin\\filter\\filter_utils.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\filter\n",
      "      copying py_stringsimjoin\\filter\\overlap_filter.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\filter\n",
      "      copying py_stringsimjoin\\filter\\position_filter.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\filter\n",
      "      copying py_stringsimjoin\\filter\\prefix_filter.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\filter\n",
      "      copying py_stringsimjoin\\filter\\size_filter.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\filter\n",
      "      copying py_stringsimjoin\\filter\\suffix_filter.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\filter\n",
      "      copying py_stringsimjoin\\filter\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\filter\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringsimjoin\\index\n",
      "      copying py_stringsimjoin\\index\\index.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\index\n",
      "      copying py_stringsimjoin\\index\\inverted_index.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\index\n",
      "      copying py_stringsimjoin\\index\\position_index.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\index\n",
      "      copying py_stringsimjoin\\index\\prefix_index.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\index\n",
      "      copying py_stringsimjoin\\index\\size_index.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\index\n",
      "      copying py_stringsimjoin\\index\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\index\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\cosine_join.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\cosine_join_py.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\dice_join.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\dice_join_py.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\disk_edit_distance_join.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\edit_distance_join.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\edit_distance_join_py.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\jaccard_join.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\jaccard_join_py.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\overlap_coefficient_join.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\overlap_coefficient_join_py.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\overlap_join.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\overlap_join_py.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\set_sim_join.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringsimjoin\\matcher\n",
      "      copying py_stringsimjoin\\matcher\\apply_matcher.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\matcher\n",
      "      copying py_stringsimjoin\\matcher\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\matcher\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringsimjoin\\profiler\n",
      "      copying py_stringsimjoin\\profiler\\profiler.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\profiler\n",
      "      copying py_stringsimjoin\\profiler\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\profiler\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringsimjoin\\similarity_measure\n",
      "      copying py_stringsimjoin\\similarity_measure\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\similarity_measure\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\n",
      "      copying py_stringsimjoin\\tests\\test_apply_matcher.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\n",
      "      copying py_stringsimjoin\\tests\\test_converter_utils.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\n",
      "      copying py_stringsimjoin\\tests\\test_edit_dist_join.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\n",
      "      copying py_stringsimjoin\\tests\\test_helper_functions.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\n",
      "      copying py_stringsimjoin\\tests\\test_join.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\n",
      "      copying py_stringsimjoin\\tests\\test_overlap_filter.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\n",
      "      copying py_stringsimjoin\\tests\\test_overlap_join.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\n",
      "      copying py_stringsimjoin\\tests\\test_position_filter.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\n",
      "      copying py_stringsimjoin\\tests\\test_prefix_filter.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\n",
      "      copying py_stringsimjoin\\tests\\test_profiler.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\n",
      "      copying py_stringsimjoin\\tests\\test_size_filter.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\n",
      "      copying py_stringsimjoin\\tests\\test_suffix_filter.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\n",
      "      copying py_stringsimjoin\\tests\\_test_disk_edit_dist_join.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\n",
      "      copying py_stringsimjoin\\tests\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringsimjoin\\utils\n",
      "      copying py_stringsimjoin\\utils\\converter.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\utils\n",
      "      copying py_stringsimjoin\\utils\\generic_helper.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\utils\n",
      "      copying py_stringsimjoin\\utils\\missing_value_handler.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\utils\n",
      "      copying py_stringsimjoin\\utils\\missing_value_handler_disk.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\utils\n",
      "      copying py_stringsimjoin\\utils\\pickle.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\utils\n",
      "      copying py_stringsimjoin\\utils\\simfunctions.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\utils\n",
      "      copying py_stringsimjoin\\utils\\token_ordering.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\utils\n",
      "      copying py_stringsimjoin\\utils\\validation.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\utils\n",
      "      copying py_stringsimjoin\\utils\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\utils\n",
      "      running egg_info\n",
      "      writing py_stringsimjoin.egg-info\\PKG-INFO\n",
      "      writing dependency_links to py_stringsimjoin.egg-info\\dependency_links.txt\n",
      "      writing requirements to py_stringsimjoin.egg-info\\requires.txt\n",
      "      writing top-level names to py_stringsimjoin.egg-info\\top_level.txt\n",
      "      reading manifest file 'py_stringsimjoin.egg-info\\SOURCES.txt'\n",
      "      reading manifest template 'MANIFEST.in'\n",
      "      warning: no files found matching '*.csv.gz' under directory 'py_stringsimjoin\\tests'\n",
      "      adding license file 'LICENSE'\n",
      "      writing manifest file 'py_stringsimjoin.egg-info\\SOURCES.txt'\n",
      "      copying py_stringsimjoin\\index\\inverted_index_cy.pyx -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\index\n",
      "      copying py_stringsimjoin\\index\\position_index_cy.pyx -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\index\n",
      "      copying py_stringsimjoin\\join\\cosine_join_cy.pyx -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\dice_join_cy.pyx -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\disk_edit_distance_join_cy.pyx -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\edit_distance_join_cy.pyx -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\jaccard_join_cy.pyx -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\overlap_coefficient_join_cy.pyx -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\overlap_join_cy.pyx -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\set_sim_join_cy.pyx -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\similarity_measure\\cosine.pyx -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\similarity_measure\n",
      "      copying py_stringsimjoin\\similarity_measure\\dice.pyx -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\similarity_measure\n",
      "      copying py_stringsimjoin\\similarity_measure\\edit_distance.pyx -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\similarity_measure\n",
      "      copying py_stringsimjoin\\similarity_measure\\jaccard.pyx -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\similarity_measure\n",
      "      copying py_stringsimjoin\\utils\\cython_utils.pyx -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\utils\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringsimjoin\\datasets\\data\n",
      "      copying py_stringsimjoin\\datasets\\data\\books_table_A.csv.gz -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\datasets\\data\n",
      "      copying py_stringsimjoin\\datasets\\data\\books_table_B.csv.gz -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\datasets\\data\n",
      "      copying py_stringsimjoin\\datasets\\data\\person_table_A.csv -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\datasets\\data\n",
      "      copying py_stringsimjoin\\datasets\\data\\person_table_B.csv -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\datasets\\data\n",
      "      copying py_stringsimjoin\\index\\inverted_index_cy.cpp -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\index\n",
      "      copying py_stringsimjoin\\index\\position_index_cy.cpp -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\index\n",
      "      copying py_stringsimjoin\\join\\cosine_join_cy.cpp -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\dice_join_cy.cpp -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\disk_edit_distance_join_cy.cpp -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\edit_distance_join_cy.cpp -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\jaccard_join_cy.cpp -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\overlap_coefficient_join_cy.cpp -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\overlap_join_cy.cpp -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\join\\set_sim_join_cy.cpp -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\join\n",
      "      copying py_stringsimjoin\\similarity_measure\\cosine.cpp -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\similarity_measure\n",
      "      copying py_stringsimjoin\\similarity_measure\\dice.cpp -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\similarity_measure\n",
      "      copying py_stringsimjoin\\similarity_measure\\edit_distance.cpp -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\similarity_measure\n",
      "      copying py_stringsimjoin\\similarity_measure\\jaccard.cpp -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\similarity_measure\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\\data\n",
      "      copying py_stringsimjoin\\tests\\data\\table_A.csv -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\\data\n",
      "      copying py_stringsimjoin\\tests\\data\\table_B.csv -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\tests\\data\n",
      "      copying py_stringsimjoin\\utils\\cython_utils.cpp -> build\\lib.win-amd64-3.8\\py_stringsimjoin\\utils\n",
      "      running build_ext\n",
      "      building 'py_stringsimjoin.index.inverted_index_cy' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for py_stringsimjoin\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [74 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-3.8\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringmatching\n",
      "      copying py_stringmatching\\utils.py -> build\\lib.win-amd64-3.8\\py_stringmatching\n",
      "      copying py_stringmatching\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringmatching\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\affine.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\bag_distance.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\cosine.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\dice.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\editex.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\generalized_jaccard.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\hamming_distance.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\hybrid_similarity_measure.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\jaccard.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\jaro.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\jaro_winkler.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\levenshtein.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\monge_elkan.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\needleman_wunsch.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\overlap_coefficient.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\partial_ratio.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\partial_token_sort.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\phonetic_similarity_measure.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\ratio.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\sequence_similarity_measure.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\similarity_measure.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\smith_waterman.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\soft_tfidf.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\soundex.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\tfidf.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\token_similarity_measure.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\token_sort.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\tversky_index.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringmatching\\tests\n",
      "      copying py_stringmatching\\tests\\test_simfunctions.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tests\n",
      "      copying py_stringmatching\\tests\\test_sim_Soundex.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tests\n",
      "      copying py_stringmatching\\tests\\test_tokenizers.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tests\n",
      "      copying py_stringmatching\\tests\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tests\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      copying py_stringmatching\\tokenizer\\alphabetic_tokenizer.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      copying py_stringmatching\\tokenizer\\alphanumeric_tokenizer.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      copying py_stringmatching\\tokenizer\\definition_tokenizer.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      copying py_stringmatching\\tokenizer\\delimiter_tokenizer.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      copying py_stringmatching\\tokenizer\\qgram_tokenizer.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      copying py_stringmatching\\tokenizer\\tokenizer.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      copying py_stringmatching\\tokenizer\\whitespace_tokenizer.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      copying py_stringmatching\\tokenizer\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      copying py_stringmatching\\similarity_measure\\cython\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      running egg_info\n",
      "      writing py_stringmatching.egg-info\\PKG-INFO\n",
      "      writing dependency_links to py_stringmatching.egg-info\\dependency_links.txt\n",
      "      writing requirements to py_stringmatching.egg-info\\requires.txt\n",
      "      writing top-level names to py_stringmatching.egg-info\\top_level.txt\n",
      "      reading manifest file 'py_stringmatching.egg-info\\SOURCES.txt'\n",
      "      reading manifest template 'MANIFEST.in'\n",
      "      warning: no files found matching 'requirements.txt'\n",
      "      adding license file 'LICENSE'\n",
      "      writing manifest file 'py_stringmatching.egg-info\\SOURCES.txt'\n",
      "      copying py_stringmatching\\similarity_measure\\cython\\cython_affine.c -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      copying py_stringmatching\\similarity_measure\\cython\\cython_jaro.c -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      copying py_stringmatching\\similarity_measure\\cython\\cython_jaro_winkler.c -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      copying py_stringmatching\\similarity_measure\\cython\\cython_levenshtein.c -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      copying py_stringmatching\\similarity_measure\\cython\\cython_needleman_wunsch.c -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      copying py_stringmatching\\similarity_measure\\cython\\cython_smith_waterman.c -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      copying py_stringmatching\\similarity_measure\\cython\\cython_utils.c -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      running build_ext\n",
      "      building 'py_stringmatching.similarity_measure.cython.cython_levenshtein' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for py_stringmatching\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Running setup.py install for py_stringmatching did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [76 lines of output]\n",
      "      running install\n",
      "      C:\\Users\\Xiaoyi WU\\.conda\\envs\\py38\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "        warnings.warn(\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-3.8\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringmatching\n",
      "      copying py_stringmatching\\utils.py -> build\\lib.win-amd64-3.8\\py_stringmatching\n",
      "      copying py_stringmatching\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringmatching\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\affine.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\bag_distance.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\cosine.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\dice.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\editex.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\generalized_jaccard.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\hamming_distance.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\hybrid_similarity_measure.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\jaccard.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\jaro.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\jaro_winkler.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\levenshtein.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\monge_elkan.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\needleman_wunsch.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\overlap_coefficient.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\partial_ratio.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\partial_token_sort.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\phonetic_similarity_measure.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\ratio.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\sequence_similarity_measure.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\similarity_measure.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\smith_waterman.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\soft_tfidf.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\soundex.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\tfidf.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\token_similarity_measure.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\token_sort.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\tversky_index.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      copying py_stringmatching\\similarity_measure\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringmatching\\tests\n",
      "      copying py_stringmatching\\tests\\test_simfunctions.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tests\n",
      "      copying py_stringmatching\\tests\\test_sim_Soundex.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tests\n",
      "      copying py_stringmatching\\tests\\test_tokenizers.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tests\n",
      "      copying py_stringmatching\\tests\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tests\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      copying py_stringmatching\\tokenizer\\alphabetic_tokenizer.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      copying py_stringmatching\\tokenizer\\alphanumeric_tokenizer.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      copying py_stringmatching\\tokenizer\\definition_tokenizer.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      copying py_stringmatching\\tokenizer\\delimiter_tokenizer.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      copying py_stringmatching\\tokenizer\\qgram_tokenizer.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      copying py_stringmatching\\tokenizer\\tokenizer.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      copying py_stringmatching\\tokenizer\\whitespace_tokenizer.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      copying py_stringmatching\\tokenizer\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\tokenizer\n",
      "      creating build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      copying py_stringmatching\\similarity_measure\\cython\\__init__.py -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      running egg_info\n",
      "      writing py_stringmatching.egg-info\\PKG-INFO\n",
      "      writing dependency_links to py_stringmatching.egg-info\\dependency_links.txt\n",
      "      writing requirements to py_stringmatching.egg-info\\requires.txt\n",
      "      writing top-level names to py_stringmatching.egg-info\\top_level.txt\n",
      "      reading manifest file 'py_stringmatching.egg-info\\SOURCES.txt'\n",
      "      reading manifest template 'MANIFEST.in'\n",
      "      warning: no files found matching 'requirements.txt'\n",
      "      adding license file 'LICENSE'\n",
      "      writing manifest file 'py_stringmatching.egg-info\\SOURCES.txt'\n",
      "      copying py_stringmatching\\similarity_measure\\cython\\cython_affine.c -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      copying py_stringmatching\\similarity_measure\\cython\\cython_jaro.c -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      copying py_stringmatching\\similarity_measure\\cython\\cython_jaro_winkler.c -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      copying py_stringmatching\\similarity_measure\\cython\\cython_levenshtein.c -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      copying py_stringmatching\\similarity_measure\\cython\\cython_needleman_wunsch.c -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      copying py_stringmatching\\similarity_measure\\cython\\cython_smith_waterman.c -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      copying py_stringmatching\\similarity_measure\\cython\\cython_utils.c -> build\\lib.win-amd64-3.8\\py_stringmatching\\similarity_measure\\cython\n",
      "      running build_ext\n",
      "      building 'py_stringmatching.similarity_measure.cython.cython_levenshtein' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "× Encountered error while trying to install package.\n",
      "╰─> py_stringmatching\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\xiaoyi wu\\.conda\\envs\\py38\\lib\\site-packages)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'py_stringsimjoin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\2022实习\\wharton\\week-3\\si485-fuzzy-matching-master\\si485-fuzzy-matching-master\\fuzzy_matching.ipynb Cell 46\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/2022%E5%AE%9E%E4%B9%A0/wharton/week-3/si485-fuzzy-matching-master/si485-fuzzy-matching-master/fuzzy_matching.ipynb#ch0000042?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install py_stringsimjoin\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/2022%E5%AE%9E%E4%B9%A0/wharton/week-3/si485-fuzzy-matching-master/si485-fuzzy-matching-master/fuzzy_matching.ipynb#ch0000042?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpy_stringsimjoin\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mssj\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'py_stringsimjoin'"
     ]
    }
   ],
   "source": [
    "!pip install py_stringsimjoin\n",
    "\n",
    "import py_stringsimjoin as ssj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fcc537-b398-4be0-b4a8-59262026f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdata_orbis.reset_index(inplace=True)\n",
    "outdata_gdelt.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b504aa-11b7-4f68-9597-4d171b6e68ff",
   "metadata": {},
   "source": [
    "#### Distance join\n",
    "\n",
    "Join two tables using edit distance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b22417d-ad50-4941-9de2-3b03b358fbe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ssj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\2022实习\\wharton\\week-3\\si485-fuzzy-matching-master\\si485-fuzzy-matching-master\\fuzzy_matching.ipynb Cell 46'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/2022%E5%AE%9E%E4%B9%A0/wharton/week-3/si485-fuzzy-matching-master/si485-fuzzy-matching-master/fuzzy_matching.ipynb#ch0000040?line=0'>1</a>\u001b[0m output_pairs_distance_join \u001b[39m=\u001b[39m ssj\u001b[39m.\u001b[39medit_distance_join(outdata_orbis, outdata_gdelt,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/2022%E5%AE%9E%E4%B9%A0/wharton/week-3/si485-fuzzy-matching-master/si485-fuzzy-matching-master/fuzzy_matching.ipynb#ch0000040?line=1'>2</a>\u001b[0m                                       \u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/2022%E5%AE%9E%E4%B9%A0/wharton/week-3/si485-fuzzy-matching-master/si485-fuzzy-matching-master/fuzzy_matching.ipynb#ch0000040?line=2'>3</a>\u001b[0m                                       \u001b[39m'\u001b[39m\u001b[39mname_clean\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mname_gdelt\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/2022%E5%AE%9E%E4%B9%A0/wharton/week-3/si485-fuzzy-matching-master/si485-fuzzy-matching-master/fuzzy_matching.ipynb#ch0000040?line=3'>4</a>\u001b[0m                                       \u001b[39m50\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/2022%E5%AE%9E%E4%B9%A0/wharton/week-3/si485-fuzzy-matching-master/si485-fuzzy-matching-master/fuzzy_matching.ipynb#ch0000040?line=4'>5</a>\u001b[0m                                       l_out_attrs\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mname_clean\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/2022%E5%AE%9E%E4%B9%A0/wharton/week-3/si485-fuzzy-matching-master/si485-fuzzy-matching-master/fuzzy_matching.ipynb#ch0000040?line=5'>6</a>\u001b[0m                                       r_out_attrs\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mname_gdelt\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ssj' is not defined"
     ]
    }
   ],
   "source": [
    "output_pairs_distance_join = ssj.edit_distance_join(outdata_orbis, outdata_gdelt,\n",
    "                                      'index', 'index', \n",
    "                                      'name_clean', 'name_gdelt', \n",
    "                                      50,\n",
    "                                      l_out_attrs=['name_clean'], \n",
    "                                      r_out_attrs=['name_gdelt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf023441-6b7f-4a66-8e53-f82833fecb40",
   "metadata": {},
   "source": [
    "#### py_stringmatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e993c-f5cc-4e6c-b9cc-47a18cd26cd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install py_stringmatching\n",
    "\n",
    "import py_stringmatching as sm\n",
    "\n",
    "ws = sm.WhitespaceTokenizer(return_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ffb33-1573-497e-8dc3-b8c642606d05",
   "metadata": {},
   "source": [
    "#### Jaccard join\n",
    "\n",
    "Join two tables using Jaccard similarity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3559b99-fd86-494d-9f70-e53a8206e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pairs_jaccard_join = ssj.jaccard_join(outdata_orbis, outdata_gdelt, \n",
    "                                             'index', 'index', \n",
    "                                             'name_clean', 'name_gdelt', \n",
    "                                             ws, 0.1, \n",
    "                                             l_out_attrs=['name_clean'], \n",
    "                                             r_out_attrs=['name_gdelt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f823b-2d79-486b-8837-877199390323",
   "metadata": {},
   "source": [
    "#### Cosine join\n",
    "\n",
    "Join two tables using a variant of cosine similarity known as Ochiai coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841d341f-4549-4779-a65d-f6dc30d3786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pairs_cosine_join = ssj.cosine_join(outdata_orbis, outdata_gdelt, \n",
    "                                             'index', 'index', \n",
    "                                             'name_clean', 'name_gdelt', \n",
    "                                             ws, 0.1, \n",
    "                                             l_out_attrs=['name_clean'], \n",
    "                                             r_out_attrs=['name_gdelt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18fb292-b3ac-4406-a632-857077ae8b19",
   "metadata": {},
   "source": [
    "#### Dice join\n",
    "\n",
    "Join two tables using Dice similarity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4eec40-5c49-4585-a985-2940d8553c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pairs_dice_join = ssj.dice_join(outdata_orbis, outdata_gdelt, \n",
    "                                             'index', 'index', \n",
    "                                             'name_clean', 'name_gdelt', \n",
    "                                             ws, 0.1, \n",
    "                                             l_out_attrs=['name_clean'], \n",
    "                                             r_out_attrs=['name_gdelt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f63cd-b91a-4de3-8f0f-79b45256a16e",
   "metadata": {},
   "source": [
    "#### Overlap join\n",
    "\n",
    "Join two tables using overlap measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24148c5c-41ef-4733-a0c7-b4f2f941ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pairs_overlap_join = ssj.overlap_join(outdata_orbis, outdata_gdelt, \n",
    "                                             'index', 'index', \n",
    "                                             'name_clean', 'name_gdelt', \n",
    "                                             ws, 0.1, \n",
    "                                             l_out_attrs=['name_clean'], \n",
    "                                             r_out_attrs=['name_gdelt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329385dd-1ce0-47ff-a223-724289ea2023",
   "metadata": {},
   "source": [
    "#### Overlap coefficient join\n",
    "\n",
    "Join two tables using overlap coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45a334-3293-431b-a4e9-c03d99c77191",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pairs_overlap_coefficient_join = ssj.overlap_coefficient_join(outdata_orbis, outdata_gdelt, \n",
    "                                             'index', 'index', \n",
    "                                             'name_clean', 'name_gdelt', \n",
    "                                             ws, 0.1, \n",
    "                                             l_out_attrs=['name_clean'], \n",
    "                                             r_out_attrs=['name_gdelt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899dff3b-f9f2-4743-a9d2-2820396decc0",
   "metadata": {},
   "source": [
    "#### Master list\n",
    "\n",
    "We take the dataframe of Orbis organization names and cross-join it with the dataframe of GDelt organization names, so that every Orbis company name has a record associating it with every GDelt organization mention. For example, if there are 2 Orbis company names and 3 GDelt article mentions, then there will be six comparisons: 3 comparisons for each of the two Orbis company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894780a-1747-4dc5-b6dc-905aa243217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To cross join, merge on a temporary key and then drop it.\n",
    "outdata_gdelt['key'] = 1\n",
    "outdata_orbis['key'] = 1\n",
    "\n",
    "master_list = pd.merge(outdata_gdelt, outdata_orbis, on='key').drop('key', 1)\n",
    "master_list.rename(columns={'name_x': 'name_gdelt', \n",
    "                             'name_original_x': 'name_original_gdelt', \n",
    "                             'name': 'name_orbis', \n",
    "                             'name_clean': 'name_clean_orbis', \n",
    "                             'name_original_y': 'name_original_orbis'}, \n",
    "                    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c3e5a5-70e2-45b5-9301-8a13aca73562",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3b0e0-5de9-45e6-8c2b-b745206b0e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list.to_csv('./output/master_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88302cd-961d-43e1-b271-3b14b1e40f49",
   "metadata": {},
   "source": [
    "#### FuzzyWuzzy and Jellyfish\n",
    "\n",
    "1) Fuzzy string matching like a boss. It uses Levenshtein Distance to calculate the differences between sequences in a simple-to-use package: https://pypi.org/project/fuzzywuzzy/\n",
    "\n",
    "2) A library for doing approximate and phonetic matching of strings: https://pypi.org/project/jellyfish/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b865c-6c73-428c-b8ac-30f060bb11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy\n",
    "!pip install jellyfish\n",
    "\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "import jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb7071e-1069-4780-b2f6-dda7627b6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = master_list\n",
    "except:\n",
    "    data = pd.read_csv('./output/master_list.csv')\n",
    "    data.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "data = data.dropna() # To prevent errors processing matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64830ba6-c33a-42bf-a2c5-557727a2d2b4",
   "metadata": {},
   "source": [
    "#### Calculate fuzz ratios and jaro-wrinkler distances.\n",
    "\n",
    "This cell calculates fuzz ratios and jaro-wrinkler distances for both spelled-out organization names and their phonetic metaphone variants. A progress bar is included to track execution of large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a820dc4-7ab3-4a08-abd1-df184cce0510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get matches of names as well as meta information.\n",
    "# This is where the heavy lifting happens.\n",
    "\n",
    "display('Match processing will take some time...')\n",
    "display(str(len(data)) + ' rows...')\n",
    "\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas() # Introduces pd.apply_progress() for progress bars.\n",
    "\n",
    "# Name comparisons. Run an apply() on two columns.\n",
    "display('Calculating fuzz ratio for names...')\n",
    "data['fuzz_ratio'] = data.progress_apply(lambda x: fuzz.ratio(x.name_gdelt, x.name_clean_orbis), axis=1)\n",
    "display('Calculating fuzz partial ratio for names...')\n",
    "data['fuzz_partial_ratio'] = data.progress_apply(lambda x: fuzz.partial_ratio(x.name_gdelt, x.name_clean_orbis), axis=1)\n",
    "display('Calculating token sort ratio for names...')\n",
    "data['fuzz_token_sort_ratio'] = data.progress_apply(lambda x: fuzz.token_sort_ratio(x.name_gdelt, x.name_clean_orbis), axis=1)\n",
    "display('Calculating jaro distance for names...')\n",
    "data['jaro_distance'] = data.progress_apply(lambda x: jellyfish.jaro_distance(x.name_gdelt, x.name_clean_orbis), axis=1)\n",
    "\n",
    "# Metaphone generation.\n",
    "display('Generating metaphones for uncleaned orbis names...')\n",
    "data['metaphone_unclean_orbis'] = data['name_orbis'].progress_apply(jellyfish.metaphone)\n",
    "display('Generating metaphones for cleaned orbis names...')\n",
    "data['metaphone_clean_orbis'] = data['name_clean_orbis'].progress_apply(jellyfish.metaphone)\n",
    "display('Generating metaphones for gdelt names...')\n",
    "data['metaphone_gdelt'] = data['name_gdelt'].progress_apply(jellyfish.metaphone)\n",
    "\n",
    "# Metaphone comparisons. Run an apply() on two columns.\n",
    "display('Calculating fuzz ratio for metaphones...')\n",
    "data['metaphone_fuzz_ratio'] = data.progress_apply(lambda x: fuzz.ratio(x.metaphone_gdelt, x.metaphone_clean_orbis), axis=1)\n",
    "display('Calculating fuzz partial ratio for metaphones...')\n",
    "data['metaphone_fuzz_partial_ratio'] = data.progress_apply(lambda x: fuzz.partial_ratio(x.metaphone_gdelt, x.metaphone_clean_orbis), axis=1)\n",
    "display('Calculating token sort ratio for metaphones...')\n",
    "data['metaphone_fuzz_token_sort_ratio'] = data.progress_apply(lambda x: fuzz.token_sort_ratio(x.metaphone_gdelt, x.metaphone_clean_orbis), axis=1)\n",
    "display('Calculating jaro distance for metaphones...')\n",
    "data['metaphone_jaro_distance'] = data.progress_apply(lambda x: jellyfish.jaro_distance(x.metaphone_gdelt, x.metaphone_clean_orbis), axis=1)\n",
    "\n",
    "display('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d7c54-9113-4311-8ef2-fb91c45f191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fb9801-69ab-4216-826b-c868c686d868",
   "metadata": {},
   "source": [
    "#### py_stringsimjoin\n",
    "\n",
    "Edit distance join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207cab1-721f-44ff-adac-2eae82358944",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, \n",
    "                output_pairs_distance_join, \n",
    "                how='outer', \n",
    "                left_on=['index_x', 'index_y'], \n",
    "                right_on=['r_index', 'l_index'])\n",
    "\n",
    "data.rename(columns={'_sim_score': 'sim_score_distance'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e47896-29b4-4591-9515-f3831610765e",
   "metadata": {},
   "source": [
    "#### py_stringmatching\n",
    "\n",
    "Jaccard join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371aa5fd-79dd-4182-a79c-77649b0e6536",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, \n",
    "                output_pairs_jaccard_join, \n",
    "                how='outer', \n",
    "                left_on=['index_x', 'index_y'], \n",
    "                right_on=['r_index', 'l_index'])\n",
    "\n",
    "data.rename(columns={'_sim_score': 'sim_score_jaccard'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e8ff08-f712-465d-9dab-6ed4014115c0",
   "metadata": {},
   "source": [
    "Cosine join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2903c35f-cfee-4b19-b1ab-7682db1f79d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, \n",
    "                output_pairs_cosine_join, \n",
    "                how='outer', \n",
    "                left_on=['index_x', 'index_y'], \n",
    "                right_on=['r_index', 'l_index'])\n",
    "\n",
    "data.rename(columns={'_sim_score': 'sim_score_cosine'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d759093-0948-4026-a158-ac82600be110",
   "metadata": {},
   "source": [
    "Dice join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc7a6e5-dea9-472b-8101-785aa1c17f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, \n",
    "                output_pairs_dice_join, \n",
    "                how='outer', \n",
    "                left_on=['index_x', 'index_y'], \n",
    "                right_on=['r_index', 'l_index'])\n",
    "\n",
    "data.rename(columns={'_sim_score': 'sim_score_dice'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d104a57-d99c-4558-982f-817ec66fd7a4",
   "metadata": {},
   "source": [
    "Overlap join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d53f7-fdd6-4d7e-98a7-b99410aef077",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, output_pairs_overlap_join, \n",
    "                how='outer', \n",
    "                left_on=['index_x', 'index_y'], \n",
    "                right_on=['r_index', 'l_index'])\n",
    "\n",
    "data.rename(columns={'_sim_score': 'sim_score_overlap'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba976e38-3f32-4cde-a032-9c00e280e492",
   "metadata": {},
   "source": [
    "Overlap coefficient join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8448c3-24b9-42e0-932a-8b06b74aeb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, \n",
    "                output_pairs_overlap_coefficient_join, \n",
    "                how='outer', \n",
    "                left_on=['index_x', 'index_y'], \n",
    "                right_on=['r_index', 'l_index'])\n",
    "\n",
    "data.rename(columns={'_sim_score': 'sim_score_overlap_coefficient'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ea9b6-ed4b-4ca5-b571-c01533e0c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./output/matches_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94c4a8-48fb-48e1-ae50-b9ca01bc9cdb",
   "metadata": {},
   "source": [
    "# Sorting\n",
    "\n",
    "Sort by \"official\" Orbis name, then by the various scores that can be easily changed for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a184c7-321b-4313-a22e-096f1059e7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c320e-66a1-440b-b901-e9a1a3f43288",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    indata = data\n",
    "except:\n",
    "    indata = pd.read_csv('./output/matches_raw.csv')\n",
    "    indata.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c988b5fa-f664-4dfb-bb28-0132ffb63cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort match data in a multindex and sort by name and score.\n",
    "df_sorted = indata.set_index(['name_original_orbis', 'name_original_gdelt'])\n",
    "df_sorted = df_sorted.sort_values(by=['name_original_orbis', \n",
    "                                      'fuzz_ratio', \n",
    "                                      'fuzz_partial_ratio', \n",
    "                                      'fuzz_token_sort_ratio'], \n",
    "                                  ascending=False)\n",
    "df_sorted = df_sorted.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71af5bb-7bde-4486-8c31-b59d2752250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.to_csv('./output/matches_sorted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b7b0f-93db-4db9-a5af-99b022820264",
   "metadata": {},
   "source": [
    "# Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07585cf9-3e98-4340-94f2-de7ae18ac942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645bed9-d61e-4dc0-9f21-2bc4b36263d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_sorted\n",
    "except:\n",
    "    indata = pd.read_csv('./output/matches_sorted.csv')\n",
    "    df_sorted = indata.set_index(['name_original_orbis', 'name_original_gdelt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26463e6b-2e69-47ce-ae17-ac5c485ccd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case we want to look at the df\n",
    "# we should have the columns in a nice order.\n",
    "\n",
    "df_unscored = df_sorted[[\n",
    "    # 'acronym_gdelt', \n",
    "    'freq_gdelt', \n",
    "    'fuzz_ratio', \n",
    "    'fuzz_partial_ratio', \n",
    "    'fuzz_token_sort_ratio', \n",
    "    'jaro_distance', \n",
    "    'metaphone_unclean_orbis', \n",
    "    'metaphone_clean_orbis', \n",
    "    'metaphone_gdelt',\n",
    "    'metaphone_jaro_distance',\n",
    "    'metaphone_fuzz_ratio',\n",
    "    'metaphone_fuzz_partial_ratio',\n",
    "    'metaphone_fuzz_token_sort_ratio',\n",
    "    'sim_score_distance',\n",
    "    'sim_score_jaccard',\n",
    "    'sim_score_cosine',\n",
    "    'sim_score_dice',\n",
    "    'sim_score_overlap',\n",
    "    'sim_score_overlap_coefficient',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac82a4-156f-4253-9560-e5a897eef872",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_unscored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b20c7f2-2ff8-4b7b-8c02-829f9ac06422",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unscored.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a118157-5547-4979-a3b2-b234f300ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress here to allow fast manipulation of filtering below.\n",
    "df_scored = df_unscored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec98f0e3-3d81-49b4-8081-561cf6896d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "`f67`1#### Calculate fuzz similarity\n",
    "\n",
    "Three fuzz scores are added into a cumulativee \"fuzz similarity\". Other scoring measures may also be introduced here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc54d9-6ee3-42fd-97f4-9bd527c0b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An approach called \"fuzz similarity\"\n",
    "# https://www.analyticsinsight.net/company-names-standardization-using-a-fuzzy-nlp-approach/\n",
    "df_scored[R8'fuzz_similarity'] = (2 * df_scored['fuzz_partial_ratio'] * df_scored['fuzz_token_sort_ratio']) / (df_scored['fuzz_partial_ratio'] + df_scored['fuzz_token_sort_ratio'])\n",
    "\n",
    "# Cumulative scores.\n",
    "df_scored['total_score_name'] = df_scored['fuzz_ratio'] + df_scored['fuzz_partial_ratio'] + df_scored['fuzz_token_sort_ratio']\n",
    "df_scored['total_score_metaphone'] = df_scored['metaphone_fuzz_ratio'] + df_scored['metaphone_fuzz_partial_ratio'] + df_scored['metaphone_fuzz_token_sort_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6470f82-e5dc-40f9-992e-3fc24ab70848",
   "metadata": {},
   "source": [
    "#### Threshold settings\n",
    "\n",
    "Change the match threshold scores to experiment with accuracy and sensitivity. You can mix and match different scores to refine results and test different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c354cbe1-1e12-418b-abcc-b43263696646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress here to allow fast manipulation of matching below.\n",
    "df_matches = df_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514fd3a-af50-49cc-8e6e-385ffeb8200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter matches.\n",
    "df_matches = df_matches[((df_matches['total_score_name'] > 280.0) & (df_matches['jaro_distance'] > 0.9))]\n",
    "\n",
    "# Additional scoring methods for experimentation:\n",
    "# df_matches = df_matches[df_matches['sim_score_distance'] <= 1]\n",
    "# df_matches = df_matches[df_matches['sim_score_jaccard'] <= 2]\n",
    "# df_matches = df_matches[df_matches['sim_score_cosine'] <= 2]\n",
    "# df_matches = df_matches[df_matches['sim_score_dice'] <= 2]\n",
    "# df_matches = df_matches[df_matches['sim_score_overlap'] <= 2]\n",
    "# df_matches = df_matches[df_matches['sim_score_overlap_coefficient'] <= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd08a7-7264-4d64-853e-914b42919888",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc29172-2610-4285-9876-cafdebe46123",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebd58d-7630-4278-be74-924b656635cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.to_csv('./output/matches_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3694d74f-dc70-437a-abac-001f843de0bd",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a04a06e-7d56-4906-922c-8a07950e8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99ecc9-d17e-4a5d-a5fe-aba0b71598ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    indata = df_matches\n",
    "except:\n",
    "    indata = pd.read_csv('./output/matches_filtered.csv')\n",
    "    indata = indata.set_index(['name_original_orbis', 'name_original_gdelt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc18b1-8175-47d9-8e2a-9f37e8b7121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the final output.\n",
    "dataout = indata[['fuzz_similarity', \n",
    "                  'total_score_name', \n",
    "                  'total_score_metaphone', \n",
    "                  'freq_gdelt', \n",
    "                  'jaro_distance', \n",
    "                  'metaphone_jaro_distance', \n",
    "                  'sim_score_distance',\n",
    "                  'sim_score_jaccard',\n",
    "                  'sim_score_cosine',\n",
    "                  'sim_score_dice',\n",
    "                  'sim_score_overlap',\n",
    "                  'sim_score_overlap_coefficient',\n",
    "                 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc7d036-a358-432b-899c-4f4ee9ab28b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataout.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf6e6c-d26e-4fd8-a6cf-7dff5aafd310",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataout.to_csv('./output/OUTPUT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc641a-05f9-444a-a20f-ebed012fb983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed3b282340ef5cd0fa4fc1b9aec12891ed6e866a77d37d64ca582285d69e3b91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
