{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quZ0naYCwpr_",
    "tags": []
   },
   "source": [
    "<h2> Introduction </h2>\n",
    "This Jupyter Notebook last updated by Vijay Yevatkar (vjyvtkr@seas.upenn.edu) builds on work done by Shrivats Agrawal (shriv9@seas.upenn.edu) and Ssuying Chen (cssuying@umich.edu) and Emily Oxford (eoxford@umich.edu) to train a BERT model.\n",
    "\n",
    "<h5> GDELT: </h5>\n",
    "The GDELT Project (Global Database of Events, Language, and Tone) is the largest, most comprehensive, and highest resolution open database of human society as of now. It connects the world's people, locations, organizations, themes, counts, images, and emotions into a single holistic network over the globe. There are various analysis tools under the GDELT Analysis Service like Event Geographic Network, GKG Heatmap, GKG Word Cloud, and GKG Thematic Timeline.\n",
    "\n",
    "<h2> Aim: </h2>\n",
    "The goal of this script is to provide an effective solution to tag datasets fetched from the AWS GDelt database with the corresponding sectors. In that pursuit a Bidirectional Encoder Representations from Transformers (BERT) model is used for predictions.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MD8z7LvsuZP"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install pip --upgrade\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install openpyxl\n",
    "!pip install ipywidgets\n",
    "!pip install smart_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bpclORXA19oi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import gc, io, re, time, openpyxl\n",
    "\n",
    "from io import BytesIO \n",
    "from smart_open import open as smart_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YEAR = \"2015\"\n",
    "# MONTH = \"03\"\n",
    "\n",
    "# #----For Reading -----\n",
    "# S3_LOCATION = \"ATHENA_BERT_RESULTS_AFRICA_2015_2023/\" + YEAR + \"/\" + MONTH\n",
    "# S3_BUCKET = 'sector-classification'\n",
    "# S3_CLIENT = boto3.client('s3')\n",
    "\n",
    "# #---For Saving------\n",
    "# file_location = \"Bert_Results_Africa_2015_2023\"\n",
    "# op_file_format = \"parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to read parquet files and return a dataframe\n",
    "# def read_parquet_file(bucket, file_key):\n",
    "#     obj = S3_CLIENT.get_object(Bucket = bucket, Key = file_key)\n",
    "#     return pd.read_parquet(io.BytesIO(obj['Body'].read()))\n",
    "\n",
    "\n",
    "# #Return a list of all parquet files in the folder\n",
    "# def return_all_files(bucket, location):\n",
    "#     my_bucket = boto3.Session().resource('s3').Bucket(bucket)\n",
    "#     files_list = []\n",
    "#     for file in my_bucket.objects.filter(Prefix = location):\n",
    "#         file_dict = {\"bucket\": file.bucket_name, \"file_key\": file.key}\n",
    "#         files_list.append(file_dict)\n",
    "#     return files_list\n",
    "\n",
    "\n",
    "#Labels Dict\n",
    "def get_labels_dict():\n",
    "    file_key = \"Data/sasb_full_training.xlsx\"\n",
    "    obj = S3_CLIENT.get_object(Bucket = S3_BUCKET, Key=file_key)\n",
    "\n",
    "    sasb_data = pd.read_excel(io.BytesIO(obj['Body'].read()), engine = 'openpyxl')\n",
    "    sasb_data.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "    labels = sasb_data.Sector.unique()\n",
    "    labels = np.delete(labels, 11)\n",
    "    \n",
    "    return {index: label for index, label in enumerate(sorted(labels))}\n",
    "\n",
    "\n",
    "# #Function to clean and generate column for prediction\n",
    "# def obtain_text(row):\n",
    "#     replace_points=['.',\"/\",\"\\n\",\"https\",\"http\",\":\",\"www\",\"  \"]\n",
    "#     source=str(row['sourcecommonname'])\n",
    "#     doc_identifier=str(row['documentidentifier'])\n",
    "#     themes=str(row['themes'])\n",
    "#     for replace_symbol in replace_points:\n",
    "#         source=source.replace(replace_symbol,\" \")\n",
    "#         doc_identifier=doc_identifier.replace(replace_symbol,\" \")\n",
    "\n",
    "#     try:\n",
    "#         themes_text= \" \".join(re.findall(\".*?theme=(.*?),\",themes))\n",
    "#     except Exception as e:\n",
    "#         themes_text = \"\"\n",
    "  \n",
    "\n",
    "#     pred_text=\" \".join([source,doc_identifier,themes_text])\n",
    "#     pred_text=pred_text.replace(\"_\",\" \")\n",
    "\n",
    "#     text_list=pred_text.split(\" \")\n",
    "#     text_list_unique=list(dict.fromkeys(text_list))\n",
    "#     pred_text= \" \".join(text_list_unique[:200])\n",
    "  \n",
    "#     return pred_text[:1000]\n",
    "\n",
    "\n",
    "#Assigning text labels to categorical numbers\n",
    "# label_dict = get_labels_dict()\n",
    "# def return_labels(label_number):\n",
    "#     global label_dict\n",
    "#     try:\n",
    "#         return label_dict[label_number]\n",
    "#     except Exception as e:\n",
    "#         return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pre_Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "#                                                       num_labels = len(label_dict),\n",
    "#                                                       output_attentions = False,\n",
    "#                                                       output_hidden_states = False)\n",
    "# model.to(device)\n",
    "# model.load_state_dict(torch.load('Bert_Models/second_finetuned_BERT_epoch_5.model', map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "#                                                       num_labels = len(label_dict),\n",
    "#                                                       output_attentions = False,\n",
    "#                                                       output_hidden_states = False)\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "# load_path = \"s3://zsguo/sagemaker-automated-execution/Bert_Models/second_finetuned_BERT_epoch_5.model\"\n",
    "# with smart_open(load_path, 'rb') as f:\n",
    "#     buffer = io.BytesIO(f.read())\n",
    "#     # model.load_state_dict(torch.load(buffer))\n",
    "#     model.load_state_dict(torch.load(buffer, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_predictions(df_batch, batch_size):\n",
    "    \n",
    "#     max_count = df_batch.shape[0]\n",
    "#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
    "#     predictions_list=[]  \n",
    "\n",
    "#     for i in tqdm(range(0, max_count, batch_size)):\n",
    "#         test_data = list(df_batch[i : i + batch_size]['pred_text'])\n",
    "#         predicted = [-1 for x in range(len(test_data))]\n",
    "#         try:\n",
    "#             with torch.cuda.amp.autocast():\n",
    "#                 encoded_data_test = tokenizer.batch_encode_plus(\n",
    "#                     test_data[:batch_size],\n",
    "#                     add_special_tokens=True, \n",
    "#                     return_attention_mask=True, \n",
    "#                     padding=True, \n",
    "#                     max_length=384,\n",
    "#                     return_tensors='pt'\n",
    "#                 )\n",
    "#             input_ids_moderna = encoded_data_test['input_ids']\n",
    "#             attention_masks_moderna = encoded_data_test['attention_mask']\n",
    "#             output_moderna = model(input_ids_moderna.to(device))\n",
    "#             _, predicted = torch.max(output_moderna[0], 1)\n",
    "#             predicted = predicted.tolist()\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "\n",
    "#         predictions_list.extend(predicted)\n",
    "\n",
    "#         if(i%(50*batch_size)==0):\n",
    "#             torch.cuda.empty_cache()\n",
    "        \n",
    "#     return predictions_list\n",
    "\n",
    "\n",
    "# #Saving df to S3\n",
    "# def save_df_to_s3(df, file_location, year, month, counter, file_format):\n",
    "    \n",
    "#     assert file_format in (\"csv\", \"parquet\"), \"File format must be in {csv, parquet}\"\n",
    "    \n",
    "#     file_name = f\"{month}{year[-2:]}_{counter}.{file_format}\"\n",
    "#     file_key = file_location + \"/\" + year + \"/\" + month + \"/\" + file_name\n",
    "#     if file_format == \"csv\":\n",
    "#         with io.StringIO() as buffer:\n",
    "#             df.to_csv(buffer, index=False)\n",
    "#             response = S3_CLIENT.put_object(Bucket = S3_BUCKET, Key = file_key, Body = buffer.getvalue())\n",
    "#     else:\n",
    "#         with io.BytesIO() as buffer:\n",
    "#             df.to_parquet(buffer, index=False)\n",
    "#             response = S3_CLIENT.put_object(Bucket = S3_BUCKET, Key = file_key, Body = buffer.getvalue())\n",
    "            \n",
    "#     status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "#     success = \"Successful\" if status == 200 else \"Unsuccessful\"\n",
    "#     print(f\"\\nCSV | {success} S3 put_object response. Status - {status}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Return a list of all parquet files in the folder\n",
    "def return_all_files(bucket, location):\n",
    "    my_bucket = boto3.Session().resource('s3').Bucket(bucket)\n",
    "    files_list = []\n",
    "    for file in my_bucket.objects.filter(Prefix = location):\n",
    "        file_dict = {\"bucket\": file.bucket_name, \"file_key\": file.key}\n",
    "        files_list.append(file_dict)\n",
    "    return files_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################################################################\n",
      " #################################2015-03################################## \n",
      "#######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "File: 1 | ATHENA_BERT_RESULTS_AFRICA_2015_2023/2015/03/20230221_051009_00095_rxfkq_01c33171-29bb-40da-a807-449ad82462f1\n",
      "df Shape: (32645, 16)\n",
      "-> Pre-proccesing file\n",
      "Preprocessing time: 0:01:02.907452\n",
      "-> Ready for prediction!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e446f2d2054fe2b9638826485cfd18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2358: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Prediction Complete. \n",
      "->Preparing to save file.\n",
      "\n",
      "CSV | Successful S3 put_object response. Status - 200\n",
      "\n",
      "-> File saved! Time for saving file: 0:00:04.615143\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "File: 2 | ATHENA_BERT_RESULTS_AFRICA_2015_2023/2015/03/20230221_051009_00095_rxfkq_05c204c0-6c61-4d83-a464-32e2cf619519\n",
      "df Shape: (33444, 16)\n",
      "-> Pre-proccesing file\n",
      "Preprocessing time: 0:01:02.859211\n",
      "-> Ready for prediction!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037e4bc2af254fe095389ff68c84f6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1046 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2358: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Prediction Complete. \n",
      "->Preparing to save file.\n",
      "\n",
      "CSV | Successful S3 put_object response. Status - 200\n",
      "\n",
      "-> File saved! Time for saving file: 0:00:04.656259\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "File: 3 | ATHENA_BERT_RESULTS_AFRICA_2015_2023/2015/03/20230221_051009_00095_rxfkq_0d3c3d4b-5292-428a-86e7-7bee360a7b2b\n",
      "df Shape: (31350, 16)\n",
      "-> Pre-proccesing file\n",
      "Preprocessing time: 0:01:00.606607\n",
      "-> Ready for prediction!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3d767bab944d42babd2232c2830430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2358: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d96f57ba62ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m#Making predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mpredictions_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-> Prediction Complete. \\n->Preparing to save file.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mtstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d96f57ba62ef>\u001b[0m in \u001b[0;36mmake_predictions\u001b[0;34m(df_batch, batch_size)\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0moutput_moderna\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids_moderna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_moderna\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                     \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop through all the years and months of data\n",
    "for yr in range(2015, 2023):\n",
    "    for mo in range(1, 13):\n",
    "        if yr == 2015 and mo < 3:\n",
    "            continue\n",
    "        if yr == 2022 and mo > 10:\n",
    "            continue\n",
    "        \n",
    "        if yr < 2020:\n",
    "            continue\n",
    "        \n",
    "        if yr == 2020 and mo < 12:\n",
    "            continue\n",
    "            \n",
    "        YEAR = str(yr)\n",
    "        MONTH = '{:02d}'.format(mo)\n",
    "        \n",
    "        #----For Reading -----\n",
    "        S3_LOCATION = \"ATHENA_BERT_RESULTS_AFRICA_2015_2023/\" + YEAR + \"/\" + MONTH\n",
    "        S3_BUCKET = 'sector-classification'\n",
    "        S3_CLIENT = boto3.client('s3')\n",
    "\n",
    "        #---For Saving------\n",
    "        file_location = \"Bert_Results_Africa_2015_2023\"\n",
    "        op_file_format = \"parquet\"\n",
    "        \n",
    "        files_list = return_all_files(S3_BUCKET, S3_LOCATION)\n",
    "        files_list = [file for file in files_list if file['file_key'] != S3_LOCATION + \"/\"]     # avoid folder itself\n",
    "\n",
    "        batch_size, df_batch = 32, None\n",
    "        \n",
    "        print(f\"\\n#######################################################################\\n #################################{YEAR}-{MONTH}################################## \\n#######################################################################\\n\")\n",
    "   \n",
    "        ###############################\n",
    "\n",
    "        # Function to read parquet files and return a dataframe\n",
    "        def read_parquet_file(bucket, file_key):\n",
    "            obj = S3_CLIENT.get_object(Bucket = bucket, Key = file_key)\n",
    "            return pd.read_parquet(io.BytesIO(obj['Body'].read()))\n",
    "\n",
    "\n",
    "#         #Return a list of all parquet files in the folder\n",
    "#         def return_all_files(bucket, location):\n",
    "#             my_bucket = boto3.Session().resource('s3').Bucket(bucket)\n",
    "#             files_list = []\n",
    "#             for file in my_bucket.objects.filter(Prefix = location):\n",
    "#                 file_dict = {\"bucket\": file.bucket_name, \"file_key\": file.key}\n",
    "#                 files_list.append(file_dict)\n",
    "#             return files_list\n",
    "\n",
    "\n",
    "        #Labels Dict\n",
    "        def get_labels_dict():\n",
    "            file_key = \"Data/sasb_full_training.xlsx\"\n",
    "            obj = S3_CLIENT.get_object(Bucket = S3_BUCKET, Key=file_key)\n",
    "\n",
    "            sasb_data = pd.read_excel(io.BytesIO(obj['Body'].read()), engine = 'openpyxl')\n",
    "            sasb_data.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "            labels = sasb_data.Sector.unique()\n",
    "            labels = np.delete(labels, 11)\n",
    "\n",
    "            return {index: label for index, label in enumerate(sorted(labels))}\n",
    "\n",
    "\n",
    "        #Function to clean and generate column for prediction\n",
    "        def obtain_text(row):\n",
    "            replace_points=['.',\"/\",\"\\n\",\"https\",\"http\",\":\",\"www\",\"  \"]\n",
    "            source=str(row['sourcecommonname'])\n",
    "            doc_identifier=str(row['documentidentifier'])\n",
    "            themes=str(row['themes'])\n",
    "            for replace_symbol in replace_points:\n",
    "                source=source.replace(replace_symbol,\" \")\n",
    "                doc_identifier=doc_identifier.replace(replace_symbol,\" \")\n",
    "\n",
    "            try:\n",
    "                themes_text= \" \".join(re.findall(\".*?theme=(.*?),\",themes))\n",
    "            except Exception as e:\n",
    "                themes_text = \"\"\n",
    "\n",
    "\n",
    "            pred_text=\" \".join([source,doc_identifier,themes_text])\n",
    "            pred_text=pred_text.replace(\"_\",\" \")\n",
    "\n",
    "            text_list=pred_text.split(\" \")\n",
    "            text_list_unique=list(dict.fromkeys(text_list))\n",
    "            pred_text= \" \".join(text_list_unique[:200])\n",
    "\n",
    "            return pred_text[:1000]\n",
    "        \n",
    "        #Assigning text labels to categorical numbers\n",
    "        label_dict = get_labels_dict()\n",
    "        def return_labels(label_number):\n",
    "            global label_dict\n",
    "            try:\n",
    "                return label_dict[label_number]\n",
    "            except Exception as e:\n",
    "                return \"\"\n",
    "            \n",
    "        def make_predictions(df_batch, batch_size):\n",
    "    \n",
    "            max_count = df_batch.shape[0]\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
    "            predictions_list=[]  \n",
    "\n",
    "            for i in tqdm(range(0, max_count, batch_size)):\n",
    "                test_data = list(df_batch[i : i + batch_size]['pred_text'])\n",
    "                predicted = [-1 for x in range(len(test_data))]\n",
    "                try:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        encoded_data_test = tokenizer.batch_encode_plus(\n",
    "                            test_data[:batch_size],\n",
    "                            add_special_tokens=True, \n",
    "                            return_attention_mask=True, \n",
    "                            padding=True, \n",
    "                            max_length=384,\n",
    "                            return_tensors='pt'\n",
    "                        )\n",
    "                    input_ids_moderna = encoded_data_test['input_ids']\n",
    "                    attention_masks_moderna = encoded_data_test['attention_mask']\n",
    "                    output_moderna = model(input_ids_moderna.to(device))\n",
    "                    _, predicted = torch.max(output_moderna[0], 1)\n",
    "                    predicted = predicted.tolist()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "                predictions_list.extend(predicted)\n",
    "\n",
    "                if(i%(50*batch_size)==0):\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            return predictions_list\n",
    "\n",
    "\n",
    "        #Saving df to S3\n",
    "        def save_df_to_s3(df, file_location, year, month, counter, file_format):\n",
    "\n",
    "            assert file_format in (\"csv\", \"parquet\"), \"File format must be in {csv, parquet}\"\n",
    "\n",
    "            file_name = f\"{month}{year[-2:]}_{counter}.{file_format}\"\n",
    "            file_key = file_location + \"/\" + year + \"/\" + month + \"/\" + file_name\n",
    "            if file_format == \"csv\":\n",
    "                with io.StringIO() as buffer:\n",
    "                    df.to_csv(buffer, index=False)\n",
    "                    response = S3_CLIENT.put_object(Bucket = S3_BUCKET, Key = file_key, Body = buffer.getvalue())\n",
    "            else:\n",
    "                with io.BytesIO() as buffer:\n",
    "                    df.to_parquet(buffer, index=False)\n",
    "                    response = S3_CLIENT.put_object(Bucket = S3_BUCKET, Key = file_key, Body = buffer.getvalue())\n",
    "\n",
    "            status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "            success = \"Successful\" if status == 200 else \"Unsuccessful\"\n",
    "            print(f\"\\nCSV | {success} S3 put_object response. Status - {status}\\n\")\n",
    "        \n",
    "        ###############################\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels = len(label_dict),\n",
    "                                                      output_attentions = False,\n",
    "                                                      output_hidden_states = False)\n",
    "        model.to(device)\n",
    "\n",
    "\n",
    "        load_path = \"s3://zsguo/sagemaker-automated-execution/Bert_Models/second_finetuned_BERT_epoch_5.model\"\n",
    "        with smart_open(load_path, 'rb') as f:\n",
    "            buffer = io.BytesIO(f.read())\n",
    "        # model.load_state_dict(torch.load(buffer))\n",
    "        model.load_state_dict(torch.load(buffer, map_location=torch.device('cpu')))\n",
    "    \n",
    "    \n",
    "    \n",
    "        ################################################\n",
    "        for j in range(len(files_list)):\n",
    "\n",
    "            for i in range(2):\n",
    "                print(\"_______________________________________________________________________________\")\n",
    "\n",
    "            file_key = files_list[j]['file_key']\n",
    "            curr_bucket = files_list[j]['bucket']\n",
    "\n",
    "\n",
    "            # avoid folder itself\n",
    "            if file_key == S3_LOCATION + \"/\":\n",
    "                continue\n",
    "\n",
    "\n",
    "            print(f\"File: {j+1} | {file_key}\")\n",
    "\n",
    "            #Memory clear step\n",
    "            del df_batch\n",
    "            for _ in range(3):\n",
    "                gc.collect()\n",
    "                time.sleep(1)\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            df_batch = read_parquet_file(curr_bucket, file_key)\n",
    "            print(\"df Shape:\",df_batch.shape)\n",
    "            print(\"-> Pre-proccesing file\")\n",
    "            tstart = datetime.now()\n",
    "            df_batch['pred_text']=df_batch.apply(obtain_text,axis=1)\n",
    "            print(\"Preprocessing time:\", datetime.now() - tstart)\n",
    "            print(\"-> Ready for prediction!\")\n",
    "\n",
    "            #Making predictions\n",
    "            predictions_list = make_predictions(df_batch, batch_size)\n",
    "            print(\"-> Prediction Complete. \\n->Preparing to save file.\")\n",
    "            tstart = datetime.now()\n",
    "            df_batch['SASB_Tag'] = predictions_list\n",
    "            df_batch['Predicted_Sector'] = df_batch['SASB_Tag'].apply(return_labels)\n",
    "            df_batch = df_batch[df_batch['SASB_Tag']!=-1].reset_index(drop=True)\n",
    "            save_df_to_s3(df_batch, file_location, YEAR, MONTH, j+1, op_file_format)\n",
    "            print(\"-> File saved! Time for saving file:\", datetime.now() - tstart)\n",
    "            time.sleep(5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YEAR = \"2015\"\n",
    "# MONTH = \"03\"\n",
    "\n",
    "# #----For Reading -----\n",
    "# S3_LOCATION = \"ATHENA_BERT_RESULTS_AFRICA_2015_2023/\" + YEAR + \"/\" + MONTH\n",
    "# S3_BUCKET = 'sector-classification'\n",
    "# S3_CLIENT = boto3.client('s3')\n",
    "\n",
    "# #---For Saving------\n",
    "# file_location = \"Bert_Results_Africa_2015_2023\"\n",
    "# op_file_format = \"parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3_LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_list = return_all_files(S3_BUCKET, S3_LOCATION)\n",
    "\n",
    "# files_list = [file for file in files_list if file['file_key'] != S3_LOCATION + \"/\"]     # avoid folder itself\n",
    "# lenfiles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, df_batch = 32, None\n",
    "\n",
    "# for j in range(len(files_list)):\n",
    "    \n",
    "#     for i in range(2):\n",
    "#         print(\"_______________________________________________________________________________\")\n",
    "\n",
    "#     file_key = files_list[j]['file_key']\n",
    "#     curr_bucket = files_list[j]['bucket']\n",
    "\n",
    "        \n",
    "#     # avoid folder itself\n",
    "#     if file_key == S3_LOCATION + \"/\":\n",
    "#         continue\n",
    "    \n",
    "    \n",
    "#     print(f\"File: {j+1} | {file_key}\")\n",
    "\n",
    "#     #Memory clear step\n",
    "#     del df_batch\n",
    "#     for _ in range(3):\n",
    "#         gc.collect()\n",
    "#         time.sleep(1)\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     df_batch = read_parquet_file(curr_bucket, file_key)\n",
    "#     print(\"df Shape:\",df_batch.shape)\n",
    "#     print(\"-> Pre-proccesing file\")\n",
    "#     tstart = datetime.now()\n",
    "#     df_batch['pred_text']=df_batch.apply(obtain_text,axis=1)\n",
    "#     print(\"Preprocessing time:\", datetime.now() - tstart)\n",
    "#     print(\"-> Ready for prediction!\")\n",
    "    \n",
    "#     #Making predictions\n",
    "#     predictions_list = make_predictions(df_batch, batch_size)\n",
    "#     print(\"-> Prediction Complete. \\n->Preparing to save file.\")\n",
    "#     tstart = datetime.now()\n",
    "#     df_batch['SASB_Tag'] = predictions_list\n",
    "#     df_batch['Predicted_Sector'] = df_batch['SASB_Tag'].apply(return_labels)\n",
    "#     df_batch = df_batch[df_batch['SASB_Tag']!=-1].reset_index(drop=True)\n",
    "#     save_df_to_s3(df_batch, file_location, YEAR, MONTH, j+1, op_file_format)\n",
    "#     print(\"-> File saved! Time for saving file:\", datetime.now() - tstart)\n",
    "#     time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "colab": {
   "collapsed_sections": [],
   "name": "Shrivats-BERT_doc_classifier.ipynb",
   "provenance": []
  },
  "instance_type": "ml.g4dn.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
