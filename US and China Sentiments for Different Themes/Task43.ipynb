{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5db3e6-99eb-4e51-a598-c909195eb89c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T22:45:30.039406Z",
     "iopub.status.busy": "2023-03-18T22:45:30.039147Z",
     "iopub.status.idle": "2023-03-18T22:46:24.003181Z",
     "shell.execute_reply": "2023-03-18T22:46:24.002555Z",
     "shell.execute_reply.started": "2023-03-18T22:45:30.039358Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405b7f3cf61740bb8b60bd9fdde7e1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1679152595079_0002</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==0.25.1\n",
      "  Using cached https://files.pythonhosted.org/packages/7e/ab/ea76361f9d3e732e114adcd801d2820d5319c23d0ac5482fa3b412db217e/pandas-0.25.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from pandas==0.25.1)\n",
      "Collecting python-dateutil>=2.6.1 (from pandas==0.25.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/36/7a/87837f39d0296e723bb9b62bbb257d0355c7f6128853c78955f57342a56d/python_dateutil-2.8.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==0.25.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==0.25.1)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-0.25.1 python-dateutil-2.8.2\n",
      "\n",
      "Collecting boto3==1.26.7\n",
      "  Using cached https://files.pythonhosted.org/packages/9d/17/bc2db32cb4cd0649586e01f1958f5c21f85d3c28c6703e2dd8174b3d44a1/boto3-1.26.7-py3-none-any.whl\n",
      "Collecting botocore<1.30.0,>=1.29.7 (from boto3==1.26.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/c7/fc/a3233f966da8df1a580a8e0e78e0be2adc57ad97c10aac303b15b7dbd0ac/botocore-1.29.94-py3-none-any.whl\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3==1.26.7)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3==1.26.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/c6/af903b5fab3f9b5b1e883f49a770066314c6dcceb589cf938d48c89556c1/s3transfer-0.6.0-py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /mnt/tmp/1679179549391-0/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.7->boto3==1.26.7)\n",
      "Collecting urllib3<1.27,>=1.25.4 (from botocore<1.30.0,>=1.29.7->boto3==1.26.7)\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/f5/890a0baca17a61c1f92f72b81d3c31523c99bec609e60c292ea55b387ae8/urllib3-1.26.15-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.7->boto3==1.26.7)\n",
      "Installing collected packages: urllib3, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.26.7 botocore-1.29.94 s3transfer-0.6.0 urllib3-1.26.15\n",
      "\n",
      "Collecting rapidfuzz\n",
      "Installing collected packages: rapidfuzz\n",
      "Successfully installed rapidfuzz-2.13.7\n",
      "\n",
      "You are using pip version 9.0.1, however version 23.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\n",
      "You are using pip version 9.0.1, however version 23.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\n",
      "You are using pip version 9.0.1, however version 23.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command."
     ]
    }
   ],
   "source": [
    "# sc.list_packages()\n",
    "sc.install_pypi_package(\"pandas==0.25.1\")\n",
    "sc.install_pypi_package(\"boto3==1.26.7\")\n",
    "sc.install_pypi_package(\"rapidfuzz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b739d0-c36f-4cbc-8ebd-e6acb10ea9fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T22:46:24.004564Z",
     "iopub.status.busy": "2023-03-18T22:46:24.004362Z",
     "iopub.status.idle": "2023-03-18T22:46:24.060388Z",
     "shell.execute_reply": "2023-03-18T22:46:24.059824Z",
     "shell.execute_reply.started": "2023-03-18T22:46:24.004535Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08234610b6274e6c9ba3183878a29b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "livy-session-1\n",
      "client\n",
      "None\n",
      "None\n",
      "spark.yarn.executor.memoryOverhead : None\n",
      "spark.sql.shuffle.partitions : None\n",
      "spark.network.timeout : None\n",
      "spark.driver.memory : 2048M"
     ]
    }
   ],
   "source": [
    "confs = [\"spark.yarn.executor.memoryOverhead\",\"spark.sql.shuffle.partitions\",\"spark.network.timeout\",\"spark.driver.memory\"]\n",
    "print(sc._conf.get('spark.app.name'))\n",
    "print(sc._conf.get('spark.submit.deployMode'))\n",
    "print(sc._conf.get(\"spark.yarn.executor.memoryOverhead\"))\n",
    "print(sc._conf.get(\"spark.sql.files.maxPartitionBytes\"))\n",
    "for conf in confs:\n",
    "    print(conf,\":\",sc._conf.get(conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69298fe2-a519-49e1-b542-b8d77b70d94a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab789e2a-863f-4619-8043-59704f270c82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T22:46:24.061881Z",
     "iopub.status.busy": "2023-03-18T22:46:24.061688Z",
     "iopub.status.idle": "2023-03-18T22:46:24.321919Z",
     "shell.execute_reply": "2023-03-18T22:46:24.321356Z",
     "shell.execute_reply.started": "2023-03-18T22:46:24.061853Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cdf788fed864ef5bcbe8ee4f69e22bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "import boto3\n",
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType, MapType, IntegerType, DoubleType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from collections import Counter\n",
    "import pyspark\n",
    "import rapidfuzz\n",
    "from rapidfuzz import fuzz\n",
    "import time\n",
    "import random\n",
    "\n",
    "# import s3fs\n",
    "# import boto3\n",
    "#How to pip install while creation of cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f58b24e0-067c-4440-950b-e801758bf96c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T22:46:24.323297Z",
     "iopub.status.busy": "2023-03-18T22:46:24.323101Z",
     "iopub.status.idle": "2023-03-18T22:46:24.374907Z",
     "shell.execute_reply": "2023-03-18T22:46:24.374355Z",
     "shell.execute_reply.started": "2023-03-18T22:46:24.323269Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36e414cb17e44a588e42ce427074003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e400ac-9742-47e3-8ac7-11246f1d63fd",
   "metadata": {},
   "source": [
    "## Task #43\n",
    "\n",
    "1. Find out the files which contain mention of BOTH US and China state, province or country.\n",
    "2. Check for the presence of broad themes and also check for the presence of sub-theme under each umbrella theme.\n",
    "3. Report the article count, average tone for each parent as well as sub theme.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b7ce6-2e38-49e9-a9d1-8c1deb998b7a",
   "metadata": {},
   "source": [
    "## 1.  Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "561d75de-47fc-4d2a-9890-aaf8abf7871d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T22:46:24.376080Z",
     "iopub.status.busy": "2023-03-18T22:46:24.375884Z",
     "iopub.status.idle": "2023-03-18T22:46:24.435284Z",
     "shell.execute_reply": "2023-03-18T22:46:24.434637Z",
     "shell.execute_reply.started": "2023-03-18T22:46:24.376052Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693cc7af20714feea98c69408b9263be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from boto3 import client\n",
    "\n",
    "def return_yearly_parquet_files(year):\n",
    "    print(f\"Return file list for the year: {year}\")\n",
    "    file_list = []\n",
    "    conn = client('s3')  # again assumes boto.cfg setup, assume AWS S3\n",
    "    for key in conn.list_objects(Bucket='kcsra', Prefix = f\"{str(year)} GDELT GKG PG\")['Contents']:\n",
    "        file = \"s3://kcsra/\"+ key['Key']\n",
    "        if \"parquet\" in file:\n",
    "            file_list.append(file)\n",
    "    return file_list\n",
    "\n",
    "def folder_parquet_files(year):\n",
    "    print(f\"Return file list for the year: {year}\")\n",
    "    file_list = []\n",
    "    conn = client('s3')  # again assumes boto.cfg setup, assume AWS S3\n",
    "    for key in conn.list_objects(Bucket='kcsra', Prefix = f\"{str(year)} GDELT GKG PG\")['Contents']:\n",
    "        file = \"s3://kcsra/\"+ key['Key']\n",
    "        if \"parquet\" in file:\n",
    "            file_list.append(file)\n",
    "    return file_list\n",
    "\n",
    "\n",
    "def return_correct_file_paths(year_file_list):\n",
    "    print(\"Returning the working parquet files ...\")\n",
    "    correct_file_paths = []\n",
    "    for i,file in enumerate(year_file_list):\n",
    "\n",
    "        try:\n",
    "            df = spark.read.parquet(file)\n",
    "            correct_file_paths.append(file)\n",
    "        except Exception as e:\n",
    "            print(f\"[{str(i+1)}/{len(year_file_list)}]\")\n",
    "            print(\"->ERROR while reading:\")\n",
    "            print(file)\n",
    "            print()\n",
    "    return correct_file_paths\n",
    "\n",
    "def return_df(correct_file_paths, req_cols, grid_id = None, prio_grid_list = None):\n",
    "    print(\"Constructing a dataframe from required file paths\")\n",
    "\n",
    "    df = spark.read.parquet(*correct_file_paths).select(*req_cols).dropDuplicates(['gkgrecordid'])\n",
    "    \n",
    "    if grid_id:\n",
    "        print(\"Filter Grid ID:\", grid_id)\n",
    "        df = df.filter(df.GRID_IDs == grid_id)\n",
    "    \n",
    "    if prio_grid_list:\n",
    "        print(\"Filter Prio Grid ID List:\", prio_grid_list)\n",
    "#         print(type(prio_grid_list))\n",
    "        df = df.filter(df.GRID_IDs.isin(prio_grid_list))\n",
    "\n",
    "    df = df.withColumn('day',dayofmonth(df.date)).withColumn('month',month(df.date))\n",
    "    df = df.dropna(subset = req_cols)\n",
    "    print(\"Total rows:\",df.count())\n",
    "    # df.printSchema()\n",
    "    return df\n",
    "\n",
    "def extract_tone(tone_string):\n",
    "    try:\n",
    "        tone_float = re.findall(\"tone=(.*?),\",str(tone_string))[0].strip(\"'\")\n",
    "        tone_float = float(tone_float)\n",
    "        return tone_float\n",
    "    except Exception as e:\n",
    "        print(\"Error extracting tone:\" + str(e))\n",
    "        print(tone_string)\n",
    "        print(\"=\"*20)\n",
    "        return None\n",
    "    \n",
    "def save_tone_in_new_col(df):\n",
    "    print(\"Saving the tone in a new column\")\n",
    "    extract_tone_udf = udf(extract_tone, DoubleType())\n",
    "    df = df.withColumn('Extracted_tone', extract_tone_udf(df['tone'])).drop(\"tone\").withColumnRenamed(\"Extracted_tone\", \"tone\")\n",
    "    return df\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def retrieve_theme_list(themes):\n",
    "    if not themes:\n",
    "        return {}\n",
    "    themes = str(themes)\n",
    "    return list(Counter(re.findall(f\"theme=(.*?),\",themes)).keys())\n",
    "\n",
    "@udf(returnType=ArrayType(StringType())) \n",
    "def retrieve_organization_list(organizations):\n",
    "    if not organizations:\n",
    "        return {}\n",
    "    organizations = str(organizations)\n",
    "    return list(Counter(re.findall(f\"organization=(.*?),\",organizations)).keys())\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def aggregate_category(category):\n",
    "    super_counter = Counter({})\n",
    "    for row in category:\n",
    "        super_counter += Counter(row)\n",
    "    return \", \".join([f\"{categ[0]}:{categ[1]}\" for categ in super_counter.most_common(20)])\n",
    "\n",
    "\n",
    "def are_strings_similar(target_str, data_str, match_threshold):\n",
    "    target_str, data_str = str(target_str).lower(), str(data_str).lower()\n",
    "    partial_match, sorted_match = fuzz.partial_ratio(target_str, data_str), fuzz.token_sort_ratio(target_str, data_str)\n",
    "\n",
    "    if max(partial_match, sorted_match) >= match_threshold:\n",
    "        # print(max(partial_match, sorted_match))\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def compare_string_matches_lists(target_list, data_list, match_threshold = 75):\n",
    "    for target_str in target_list:\n",
    "        for data_str in data_list:\n",
    "            if are_strings_similar(target_str, data_str, match_threshold):\n",
    "                # print(\"Match Found\", f\"| {target_str} : {data_str}\")\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def udf_compare_strings_lists(target_list, match_threshold):\n",
    "    return udf(lambda data_list: compare_string_matches_lists(target_list, data_list, match_threshold))\n",
    "\n",
    "@udf(returnType=IntegerType()) \n",
    "def check_presence_of_main_theme(*sub_theme_cols):\n",
    "    presence = 0\n",
    "    for sub_theme_col_count in sub_theme_cols:\n",
    "        print(\"Sub theme col count\")\n",
    "        presence += sub_theme_col_count\n",
    "    \n",
    "    if presence > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad69973-d593-433e-a26e-ab00e4238a46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T22:46:24.436429Z",
     "iopub.status.busy": "2023-03-18T22:46:24.436234Z",
     "iopub.status.idle": "2023-03-18T22:46:24.488753Z",
     "shell.execute_reply": "2023-03-18T22:46:24.488182Z",
     "shell.execute_reply.started": "2023-03-18T22:46:24.436401Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b19880f5244488e9664163b86ea0964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Political', 'Economic', 'Tech', 'interstate conflict', 'nationalism', 'economic integration', 'governance uncertainty', 'climate response', 'economic stagnation', 'pandemic response', 'innovation', 'green technology', 'digitalization']"
     ]
    }
   ],
   "source": [
    "themes_dict = {'Political': ['interstate conflict', 'nationalism'],\n",
    " 'Economic': ['economic integration',\n",
    "  'governance uncertainty',\n",
    "  'climate response',\n",
    "  'economic stagnation',\n",
    "  'pandemic response'],\n",
    " 'Tech': ['innovation', 'green technology', 'digitalization']}\n",
    "\n",
    "all_themes = list(themes_dict.keys())\n",
    "\n",
    "for v in list(themes_dict.values()):\n",
    "    all_themes.extend(v)\n",
    "all_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067fffe-4ec5-45b1-a52e-cc8bcbf58609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00ca0057-e859-49c0-ad89-b628921edc6e",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2938f28b-27aa-4340-aab7-05d31d62db56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T22:46:24.489755Z",
     "iopub.status.busy": "2023-03-18T22:46:24.489564Z",
     "iopub.status.idle": "2023-03-18T22:46:24.543891Z",
     "shell.execute_reply": "2023-03-18T22:46:24.543306Z",
     "shell.execute_reply.started": "2023-03-18T22:46:24.489727Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee3589c5be947ffb18695ada1800e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# match_str = \"\"\"[Google, Ballistic Research Centre, Institute Of Forensic Sciences, Torrent Pharmaceuticals, Bajaj Alliance, Directorate Of Forensic Sciences, Institute Of Behavioral Sciences, Gujarat Forensic Sciences University, Formula Group, Institute Of Research Development, Congress Mp Shashi Tharoor, Telecom Day, Us Central Intelligence Agency, Cadila Pharmaceuticals, Directorate Of Forensic Science Laboratory]\"\"\"\n",
    "\n",
    "# data_list = [x.strip() for x in match_str.strip().strip(\"[\").strip(\"]\").split(\",\")]\n",
    "# compare_string_matches_lists(prim_firm_list, data_list, match_threshold = 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaf50a8-e18f-4a33-a206-b19ea87a3eae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T22:46:24.545544Z",
     "iopub.status.busy": "2023-03-18T22:46:24.545338Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229a5e2bb1e84cbb8578a16807f8306f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acd82567bea48ddae3c228e480d27cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "match_threshold = 80\n",
    "req_cols = ['gkgrecordid','date','tone', 'year', 'themes']\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "cols_list_ordering = ['year','month',\"TotalArticles\",\"AverageSentiment\"]\n",
    "for theme in all_themes:\n",
    "    theme_total_articles = theme.replace(\" \",\"_\") + \"_TotalArticles\"\n",
    "    average_sentiment_theme = \"AverageSentiment_\" + theme.replace(\" \",\"_\")\n",
    "    cols_list_ordering.append(theme_total_articles)\n",
    "    cols_list_ordering.append(average_sentiment_theme)\n",
    "    \n",
    "    \n",
    "for year in range(2017, 2024):\n",
    "    t1 = time.time()\n",
    "    print(f\"Year: {year}\")\n",
    "#     file_list = return_yearly_parquet_files(year)\n",
    "#     file_list2 = [file for file in file_list if country in file]\n",
    "    \n",
    "    correct_file_paths = [f\"s3://shrivats-dev/Data/#43_US_CH/{year}/\"]\n",
    "#     correct_file_paths = return_correct_file_paths(file_list2)\n",
    "    df = return_df(correct_file_paths, req_cols)\n",
    "    df = save_tone_in_new_col(df)\n",
    "#     break\n",
    "    df = df.dropna(subset = [\"tone\"])\n",
    "    t2 = time.time()\n",
    "\n",
    "    print(\"Time to read files:\", t2-t1)\n",
    "    df2 = df.withColumn(\"Theme_list\", retrieve_theme_list(F.col(\"themes\")))\n",
    "    # df.unpersist()\n",
    "    for k,v in themes_dict.items():\n",
    "        for sub_theme in v:\n",
    "            sub_theme_col_count = sub_theme.replace(\" \",\"_\")+\"_count\"\n",
    "            sub_theme_col_tone = sub_theme.replace(\" \",\"_\")+\"_tone\"\n",
    "            df2 = df2.withColumn(sub_theme_col_count, udf_compare_strings_lists([sub_theme], match_threshold)(F.col(\"Theme_list\")).cast(IntegerType()))\n",
    "            df2 = df2.withColumn(sub_theme_col_tone, F.col(sub_theme_col_count) * F.col(\"tone\"))\n",
    "\n",
    "        main_theme_col_count = k.replace(\" \",\"_\") + \"_count\"\n",
    "        main_theme_col_tone = k.replace(\" \",\"_\") + \"_tone\"\n",
    "\n",
    "        sub_theme_cols = [F.col(sub_theme.replace(\" \",\"_\")+\"_count\") for sub_theme in v] \n",
    "        df2 = df2.withColumn(main_theme_col_count,check_presence_of_main_theme(*sub_theme_cols))\n",
    "        df2 = df2.withColumn(main_theme_col_tone, F.col(main_theme_col_count) * F.col(\"tone\"))\n",
    "    \n",
    "    t3 = time.time()\n",
    "    print(\"T3 time:\", t3-t2)\n",
    "   \n",
    "    \n",
    "    # df3.unpersist()\n",
    "    aggregations = []\n",
    "    for k,v in themes_dict.items():\n",
    "        main_theme_col_count = k.replace(\" \",\"_\") + \"_count\"\n",
    "        main_theme_col_tone = k.replace(\" \",\"_\") + \"_tone\"\n",
    "        aggregations.append(F.sum(main_theme_col_count).alias(k.replace(\" \",\"_\") + \"_TotalArticles\"))\n",
    "        aggregations.append(F.sum(main_theme_col_tone).alias(k.replace(\" \",\"_\") + \"_ToneSum\"))\n",
    "        for sub_theme in v:\n",
    "            sub_theme_col_count = sub_theme.replace(\" \",\"_\")+\"_count\"\n",
    "            sub_theme_col_tone = sub_theme.replace(\" \",\"_\")+\"_tone\"\n",
    "            aggregations.append(F.sum(sub_theme_col_count).alias(sub_theme.replace(\" \",\"_\") + \"_TotalArticles\"))\n",
    "            aggregations.append(F.sum(sub_theme_col_tone).alias(sub_theme.replace(\" \",\"_\") + \"_ToneSum\"))\n",
    "    \n",
    "    print(aggregations)\n",
    "    aggregation_cols = ['month','year']\n",
    "    df3 = df2.groupby(*aggregation_cols).agg(F.count(F.col(\"tone\")).alias('TotalArticles'),\\\n",
    "                              F.avg('tone').alias('AverageSentiment'),\\\n",
    "                              *aggregations)\n",
    "    \n",
    "    for theme in all_themes:\n",
    "        theme_tone_sum = theme.replace(\" \",\"_\") + \"_ToneSum\"\n",
    "        theme_total_articles = theme.replace(\" \",\"_\") + \"_TotalArticles\"\n",
    "        average_sentiment_theme = \"AverageSentiment_\" + theme.replace(\" \",\"_\")\n",
    "        df3 = df3.withColumn(average_sentiment_theme,F.col(theme_tone_sum)/F.col(theme_total_articles))\\\n",
    "                    .drop(theme_tone_sum)\n",
    "    \n",
    "    df3 = df3.fillna(value=0).select(*cols_list_ordering)\n",
    "    t4 = time.time()\n",
    "    print(\"T4 time:\", t4-t3)\n",
    "\n",
    "    df3.repartition(1).write.option(\"header\",True) \\\n",
    "       .csv(f\"s3://shrivats-dev/Tasks/#43_US_CH/Monthly/{year}\")\n",
    "    t5 = time.time()\n",
    "    print(\"T5 time:\", t5-t4)\n",
    "    print(\"=\"*50)\n",
    "\n",
    "#     break\n",
    "print(\"Total time\", time.time()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a39100-0f6e-4bc7-9688-8d50fb3e1591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols_list_ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6321eec-38ad-451d-a971-ccf1cacc59a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-13T23:51:29.605408Z",
     "iopub.status.busy": "2022-11-13T23:51:29.605138Z",
     "iopub.status.idle": "2022-11-13T23:51:29.661693Z",
     "shell.execute_reply": "2022-11-13T23:51:29.661103Z",
     "shell.execute_reply.started": "2022-11-13T23:51:29.605378Z"
    }
   },
   "source": [
    "## Joining Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "34dad5ef-f274-4c72-b801-b1939a33a16e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T19:47:21.763401Z",
     "iopub.status.busy": "2023-03-18T19:47:21.763171Z",
     "iopub.status.idle": "2023-03-18T19:47:21.816638Z",
     "shell.execute_reply": "2023-03-18T19:47:21.815979Z",
     "shell.execute_reply.started": "2023-03-18T19:47:21.763370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569e900667aa4d408c9fccb0d6230d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder = \"#43_US_CH\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd409fe1-6a31-4792-8f65-1b3185713a32",
   "metadata": {},
   "source": [
    "### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5e28e08a-1406-409d-83ce-cf3bd1f9be4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-29T19:34:52.950940Z",
     "iopub.status.busy": "2023-01-29T19:34:52.950699Z",
     "iopub.status.idle": "2023-01-29T19:35:04.262569Z",
     "shell.execute_reply": "2023-01-29T19:35:04.261982Z",
     "shell.execute_reply.started": "2023-01-29T19:34:52.950908Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317951bf3c0548239f8e6b518dee44fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_combined_all = spark.read.option(\"header\",True) \\\n",
    "               .csv(f\"s3://shrivats-dev/Tasks/{folder}/*/2*/*.csv\")\n",
    "df_combined_all = df_combined_all.withColumn(\"month\", F.col(\"month\").cast(IntegerType()))\n",
    "df_combined_all_sorted = df_combined_all.sort(\"year\",\"month\")\n",
    "df_combined_all_sorted.repartition(1).write.option(\"header\",True) \\\n",
    "               .csv(f\"s3://shrivats-dev/Tasks/{folder}/All\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e583e-2833-40c7-bf6e-57abd9a38429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
