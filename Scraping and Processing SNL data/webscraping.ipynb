{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "The objective is to create a spatial time series dataset of European Bank for Reconstruction and Development (EBRD) projects. \n",
    "\n",
    "Firstly, the code wrangles folllowing information from 4000+ EBRD project htmls: https://www.ebrd.com/work-with-us/project-finance/project-summary-documents.html\n",
    "\n",
    "        1. Publication Date\n",
    "        2. ProjectID\n",
    "        3. Country\n",
    "        4. Title\n",
    "        5. Sector\n",
    "        6. Project Type\n",
    "        7. Project Status\n",
    "        8. Link\n",
    "        \n",
    "        and additionally :\n",
    "        9. Environmental Category\n",
    "        10. Approval Date\n",
    "        11. PSD disclosed date\n",
    "        12. Project Description\n",
    "        13. Total Project Cost\n",
    "        14. location(currently, there is no location data on web)\n",
    "\n",
    "\n",
    "Secondly, the code uses the geographic location to identify latitude/longitude, using the WorldCities database on Box.\n",
    "\n",
    "Thirdly, the code uses the latitude/longitude to assign PRIO GRID IDs.\n",
    " \n",
    "Also, this code creates an indicator variable for whether the most specific location mentioned is at the country-level, as indicated in the toy dataset. \n",
    "\n",
    "Note: Do NOT add PRIO GRIDS unless there is a location more precise than country level!!\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "from lxml import html\n",
    "import time\n",
    "from lxml import etree\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC  \n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.common.exceptions import NoSuchElementException,TimeoutException\n",
    "from lxml.html.soupparser import fromstring\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initial Web Scraping & Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Publication Date', ' ProjectID', ' Country', ' Title', ' Sector',\n",
      "       ' Project Type', ' Project Status', ' Link'],\n",
      "      dtype='object')\n",
      "(4401, 8)\n",
      "(4332, 8)\n"
     ]
    }
   ],
   "source": [
    "# read raw data\n",
    "raw_df = pd.read_csv(\"../input/downloadfile.csv\")\n",
    "print(raw_df.columns)\n",
    "print(raw_df.shape)\n",
    "raw_df.drop_duplicates(subset=[\" ProjectID\"], inplace=True)\n",
    "print(raw_df.shape)\n",
    "raw_df[\" ProjectID\"] = raw_df[\" ProjectID\"].astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = [] \n",
    "wrong_html_problem_ls = [] \n",
    "# some links in raw dataset need to be modified before scraping \n",
    "def generate_link(raw_link): \n",
    "    if raw_link.startswith(\"https\"):\n",
    "        l = raw_link\n",
    "        link.append(l)\n",
    "    else:\n",
    "        if raw_link.startswith(\"/cs\") or raw_link.startswith(\"/sites\"):\n",
    "            wrong_html_problem_ls.append(raw_link)\n",
    "        else:\n",
    "            l = \"https:\" + raw_link\n",
    "            link.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4320\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "raw_df[\" Link\"].apply(lambda x: generate_link(x))\n",
    "print(len(link))\n",
    "print(len(wrong_html_problem_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_html = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_html = []\n",
    "chucksize = 100 \n",
    "for iteration in range(0,44):\n",
    "    # list stored all results for all htmls\n",
    "    project_id_ls = []\n",
    "    env_cagetory_ls = []\n",
    "    approval_date_ls = []\n",
    "    psd_disclosed_ls = []\n",
    "    project_des_ls = []\n",
    "    total_cost_ls = []\n",
    "    location_ls = []\n",
    "    \n",
    "    for html in link[iteration*chucksize:(iteration+1)*chucksize]:\n",
    "        try:\n",
    "            req = requests.get(html)\n",
    "            soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "            dom = etree.HTML(str(soup))\n",
    "\n",
    "            location_count = 0\n",
    "            project_id_count = 0\n",
    "            env_cagetory_count = 0 \n",
    "            approval_date_count = 0 \n",
    "            psd_disclosed_count = 0 \n",
    "            project_des_count = 0\n",
    "            total_cost_count = 0 \n",
    "\n",
    "            legend_ls = soup.find_all(\"legend\")\n",
    "            length_1 = len(soup.find_all(\"legend\"))\n",
    "\n",
    "            for j in range(0,length_1):\n",
    "                legend_title = legend_ls[j].text.strip().lower()\n",
    "                value = legend_ls[j].find_next().text.strip()\n",
    "\n",
    "                if legend_title.startswith(\"location\"): \n",
    "                    location_ls.append(value)\n",
    "                    location_count = 1\n",
    "                    continue\n",
    "\n",
    "                if legend_title.startswith(\"project number\"): \n",
    "                    project_id_ls.append(value)\n",
    "                    project_id_count = 1\n",
    "                    continue\n",
    "\n",
    "                if legend_title.startswith(\"environmental category\"): \n",
    "                    env_cagetory_ls.append(value)\n",
    "                    env_cagetory_count = 1\n",
    "                    continue\n",
    "\n",
    "                if legend_title.startswith(\"approval date\"): \n",
    "                    approval_date_ls.append(value)\n",
    "                    approval_date_count = 1\n",
    "                    continue\n",
    "\n",
    "                if legend_title.startswith(\"psd disclosed\"): \n",
    "                    psd_disclosed_ls.append(value)  \n",
    "                    psd_disclosed_count = 1\n",
    "                    continue\n",
    "\n",
    "            h2_ls = soup.find_all(\"article\")[0].find_all(\"h2\")\n",
    "            lenth_2 = len(h2_ls)\n",
    "            for m in range(0, lenth_2):\n",
    "                h2_title = h2_ls[m].text.strip().lower() \n",
    "    \n",
    "                if h2_title.startswith(\"project description\"):\n",
    "                    des = h2_ls[m].find_next().text.strip()\n",
    "                    project_des_ls.append(des)\n",
    "                    project_des_count = 1\n",
    "                    continue    \n",
    "\n",
    "                if h2_title.startswith(\"project cost\") or h2_title.startswith(\"total project cost\"):\n",
    "                    cost = h2_ls[m].find_next().text.strip()     \n",
    "                    total_cost_ls.append(cost)\n",
    "                    total_cost_count = 1\n",
    "                    continue  \n",
    "            \n",
    "            # if can't find any information            \n",
    "            if location_count == 0:\n",
    "                location_count.append(np.nan)\n",
    "            if project_id_count == 0:\n",
    "                problem_html.append(html)\n",
    "            if env_cagetory_count == 0:\n",
    "                env_cagetory_ls.append(np.nan)\n",
    "            if approval_date_count == 0:\n",
    "                approval_date_ls.append(np.nan)\n",
    "            if psd_disclosed_count == 0:\n",
    "                psd_disclosed_ls.append(np.nan)\n",
    "            if project_des_count == 0: \n",
    "                project_des_ls.append(np.nan)\n",
    "            if total_cost_count == 0:\n",
    "                total_cost_ls.append(np.nan)\n",
    "\n",
    "        except:   \n",
    "            print(html) # print the wrong html\n",
    "            problem_html.append(html)\n",
    "            project_id_ls.append(html)\n",
    "            location_ls.append(np.nan)\n",
    "            env_cagetory_ls.append(np.nan)\n",
    "            approval_date_ls.append(np.nan)\n",
    "            psd_disclosed_ls.append(np.nan)\n",
    "            project_des_ls.append(np.nan)\n",
    "            total_cost_ls.append(np.nan)\n",
    "            \n",
    "    d = {\n",
    "        \"Location\": location_ls, \n",
    "        \"Project number\": project_id_ls, \n",
    "        \"Environmental category\":env_cagetory_ls, \n",
    "        \"Approval date\": approval_date_ls,\n",
    "        \"PSD disclosed\": psd_disclosed_ls,\n",
    "        \"Project description\": project_des_ls,\n",
    "        \"Total project cost\": total_cost_ls,\n",
    "        \"Link\": link[iteration*chucksize:(iteration+1)*chucksize], \n",
    "     }\n",
    "    \n",
    "    df = pd.DataFrame(data = d)\n",
    "    store_path = \"../output/webscraping_\"+str(iteration)+\".csv\"\n",
    "    df.to_csv(store_path)\n",
    "    time.sleep(random.randint(4,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.ebrd.com/work-with-us/projects/psd/burgas-water-company.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location:\n",
      "Turkey\n",
      "project number:\n",
      "48387\n",
      "business sector:\n",
      "Energy\n",
      "notice type:\n",
      "Private\n",
      "environmental category:\n",
      "B\n",
      "approval date:\n",
      "19 Oct 2016\n",
      "status:\n",
      "Repaying\n",
      "psd disclosed:\n",
      "08 Jun 2016\n",
      "<p align=\"LEFT\">\n",
      "<b><font face=\"Arial\" size=\"5\"><font face=\"Arial\" size=\"5\">Project Description</font></font></b></p>\n",
      "des: project description :  A senior loan to Trakya electricity distribution company (TREDAS), Trakya electricity retail company (TREPAS), and to their parent company, IC Ictas Elektrik (ICEL) (together the co-borrowers). The transaction is part of an approximately USD 685 million dual currency financing package to be used for (i) refinancing existing financing package (acquisition of TREDAS, working capital and existing CAPEX) and (ii) financing new CAPEX for the 2016-2020 tariff period. /n\n",
      "total project cost\n",
      "des: total project cost :  USD 684,250,000.00 /n\n"
     ]
    }
   ],
   "source": [
    "# verification with single html\n",
    "\n",
    "project_id_ls = []\n",
    "env_cagetory_ls = []\n",
    "approval_date_ls = []\n",
    "psd_disclosed_ls = []\n",
    "project_des_ls = []\n",
    "total_cost_ls = []\n",
    "location_ls = []\n",
    "\n",
    "req = requests.get(\"https://www.ebrd.com/work-with-us/projects/psd/tredas-financing.html\")\n",
    "soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "dom = etree.HTML(str(soup))\n",
    "env_cagetory_count = 0 \n",
    "approval_date_count = 0 \n",
    "psd_disclosed_count = 0 \n",
    "project_des_count = 0\n",
    "total_cost_count = 0 \n",
    "\n",
    "length_1 = len(soup.find_all(\"legend\"))\n",
    "legend_ls = soup.find_all(\"legend\")\n",
    "\n",
    "for j in range(0,length_1):\n",
    "    legend_title = legend_ls[j].text.strip().lower()\n",
    "    value = legend_ls[j].find_next().text.strip()\n",
    "    print(legend_title)\n",
    "    print(value)\n",
    "    \n",
    "    if legend_title.startswith(\"project number\"): \n",
    "        project_id_ls.append(value)\n",
    "        project_id_count = 1\n",
    "        continue\n",
    "\n",
    "    if legend_title.startswith(\"environmental category\"): \n",
    "        env_cagetory_ls.append(value)\n",
    "        env_cagetory_count = 1\n",
    "        continue\n",
    "\n",
    "    if legend_title.startswith(\"approval date\"): \n",
    "        approval_date_ls.append(value)\n",
    "        approval_date_count = 1\n",
    "        continue\n",
    "\n",
    "    if legend_title.startswith(\"psd disclosed\"): \n",
    "        psd_disclosed_ls.append(value)  \n",
    "        psd_disclosed_count = 1\n",
    "        continue\n",
    "\n",
    "\n",
    "h2_ls = soup.find_all(\"article\")[0].find_all(\"p\")\n",
    "lenth_2 = len(h2_ls)\n",
    "for m in range(0, lenth_2):\n",
    "    h2_title = h2_ls[m].text.strip().lower()\n",
    "    if h2_title.startswith(\"project description\"):\n",
    "        print(h2_ls[m])\n",
    "        des = h2_ls[m].find_next_sibling('p').text.strip()\n",
    "        print(\"des:\", h2_title, \": \", des,\"/n\")    \n",
    "        continue\n",
    "\n",
    "    if h2_title.startswith(\"project cost\") or h2_title.startswith(\"total project cost\"):\n",
    "        print(h2_title)\n",
    "        cost = h2_ls[m].find_next_sibling('p').text.strip()  \n",
    "        print(\"des:\", h2_title, \": \", cost,\"/n\")    \n",
    "        continue\n",
    "    \n",
    "# d = {\"Project number\": project_id_ls, \n",
    "#      \"Environmental category\":env_cagetory_ls, \n",
    "#      \"Approval date\": approval_date_ls,\n",
    "#      \"PSD disclosed\": psd_disclosed_ls,\n",
    "#      \"Project description\": project_des_ls,\n",
    "#      \"Total cost\": total_cost_ls\n",
    "#      }\n",
    "# df = pd.DataFrame(data = d)\n",
    "# df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Refining Web Scraping results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../output/webscraping_10.csv', '../output/webscraping_38.csv', '../output/webscraping_39.csv', '../output/webscraping_11.csv', '../output/webscraping_13.csv', '../output/webscraping_12.csv', '../output/webscraping_16.csv', '../output/webscraping_17.csv', '../output/webscraping_29.csv', '../output/webscraping_15.csv', '../output/webscraping_14.csv', '../output/webscraping_28.csv', '../output/webscraping_2.csv', '../output/webscraping_3.csv', '../output/webscraping_1.csv', '../output/webscraping_49.csv', '../output/webscraping_4.csv', '../output/webscraping_5.csv', '../output/webscraping_48.csv', '../output/webscraping_7.csv', '../output/webscraping_6.csv', '../output/webscraping_52.csv', '../output/webscraping_46.csv', '../output/webscraping_47.csv', '../output/webscraping_53.csv', '../output/webscraping_45.csv', '../output/webscraping_51.csv', '../output/webscraping_8.csv', '../output/webscraping_9.csv', '../output/webscraping_50.csv', '../output/webscraping_44.csv', '../output/webscraping_40.csv', '../output/webscraping_54.csv', '../output/webscraping_55.csv', '../output/webscraping_41.csv', '../output/webscraping_43.csv', '../output/webscraping_42.csv', '../output/webscraping_31.csv', '../output/webscraping_25.csv', '../output/webscraping_19.csv', '../output/webscraping_18.csv', '../output/webscraping_24.csv', '../output/webscraping_30.csv', '../output/webscraping_26.csv', '../output/webscraping_32.csv', '../output/webscraping_33.csv', '../output/webscraping_27.csv', '../output/webscraping_23.csv', '../output/webscraping_37.csv', '../output/webscraping_36.csv', '../output/webscraping_22.csv', '../output/webscraping_34.csv', '../output/webscraping_20.csv', '../output/webscraping_21.csv', '../output/webscraping_35.csv']\n"
     ]
    }
   ],
   "source": [
    "# read web scraping results\n",
    "path = r'../output/' # use your path\n",
    "all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "print(all_files)\n",
    "ls = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    ls.append(df)\n",
    "# concate the result to one df\n",
    "pre_output_file = pd.concat(ls, axis=0, ignore_index=True).drop(columns=[\"Unnamed: 0\"])\n",
    "print(pre_output_file.shape)\n",
    "pre_output_file.head()\n",
    "pre_output_file.to_csv(\"../refine_output/pre_output_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the results where there are no project description\n",
    "no_des_data = pre_output_file[pre_output_file[ \"Project description\"].isna()]\n",
    "no_des_data.to_csv(\"../refine_output/no_description.csv\")\n",
    "no_des_link  = no_des_data[\"Link\"].to_list()\n",
    "print(len(no_des_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the result again for non-description results\n",
    "problem_html = []\n",
    "\n",
    "# list stored all results for all htmls\n",
    "project_id_ls = []\n",
    "env_cagetory_ls = []\n",
    "approval_date_ls = []\n",
    "psd_disclosed_ls = []\n",
    "project_des_ls = []\n",
    "total_cost_ls = []\n",
    "location_ls = []\n",
    "\n",
    "for html in no_des_link:\n",
    "    try:\n",
    "        req = requests.get(html)\n",
    "        soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "        dom = etree.HTML(str(soup))\n",
    "\n",
    "        location_count = 0\n",
    "        project_id_count = 0\n",
    "        env_cagetory_count = 0 \n",
    "        approval_date_count = 0 \n",
    "        psd_disclosed_count = 0 \n",
    "        project_des_count = 0\n",
    "        total_cost_count = 0 \n",
    "\n",
    "        legend_ls = soup.find_all(\"legend\")\n",
    "        length_1 = len(soup.find_all(\"legend\"))\n",
    "\n",
    "        for j in range(0,length_1):\n",
    "            legend_title = legend_ls[j].text.strip().lower()\n",
    "            value = legend_ls[j].find_next().text.strip()\n",
    "\n",
    "            if legend_title.startswith(\"location\"): \n",
    "                location_ls.append(value)\n",
    "                location_count = 1\n",
    "                continue\n",
    "\n",
    "            if legend_title.startswith(\"project number\"): \n",
    "                project_id_ls.append(value)\n",
    "                project_id_count = 1\n",
    "                continue\n",
    "\n",
    "            if legend_title.startswith(\"environmental category\"): \n",
    "                env_cagetory_ls.append(value)\n",
    "                env_cagetory_count = 1\n",
    "                continue\n",
    "\n",
    "            if legend_title.startswith(\"approval date\"): \n",
    "                approval_date_ls.append(value)\n",
    "                approval_date_count = 1\n",
    "                continue\n",
    "\n",
    "            if legend_title.startswith(\"psd disclosed\"): \n",
    "                psd_disclosed_ls.append(value)  \n",
    "                psd_disclosed_count = 1\n",
    "                continue\n",
    "        \n",
    "        # change where happens in webs\n",
    "        p_ls = soup.find_all(\"article\")[0].find_all(\"p\")\n",
    "        lenth_2 = len(p_ls)\n",
    "        for m in range(0, lenth_2):\n",
    "            h2_title = p_ls[m].text.strip().lower() \n",
    "\n",
    "            if h2_title.startswith(\"project description\"):\n",
    "                des =  p_ls[m].find_next_sibling(\"p\").text.strip() # change where happens\n",
    "                project_des_ls.append(des)\n",
    "                project_des_count = 1\n",
    "                continue    \n",
    "\n",
    "            if h2_title.startswith(\"project cost\") or h2_title.startswith(\"total project cost\"):\n",
    "                cost = p_ls[m].find_next_sibling(\"p\").text.strip()     \n",
    "                total_cost_ls.append(cost)\n",
    "                total_cost_count = 1\n",
    "                continue  \n",
    "        \n",
    "        # if can't find any information            \n",
    "        if location_count == 0:\n",
    "            location_count.append(np.nan)\n",
    "        if project_id_count == 0:\n",
    "            problem_html.append(html)\n",
    "        if env_cagetory_count == 0:\n",
    "            env_cagetory_ls.append(np.nan)\n",
    "        if approval_date_count == 0:\n",
    "            approval_date_ls.append(np.nan)\n",
    "        if psd_disclosed_count == 0:\n",
    "            psd_disclosed_ls.append(np.nan)\n",
    "        if project_des_count == 0: \n",
    "            project_des_ls.append(np.nan)\n",
    "        if total_cost_count == 0:\n",
    "            total_cost_ls.append(np.nan)\n",
    "\n",
    "    except:   \n",
    "        print(html) # print the wrong html\n",
    "        problem_html.append(html)\n",
    "        project_id_ls.append(html)\n",
    "        location_ls.append(np.nan)\n",
    "        env_cagetory_ls.append(np.nan)\n",
    "        approval_date_ls.append(np.nan)\n",
    "        psd_disclosed_ls.append(np.nan)\n",
    "        project_des_ls.append(np.nan)\n",
    "        total_cost_ls.append(np.nan)\n",
    "\n",
    "d = {\"Project number\": project_id_ls, \n",
    "     \"Environmental category\":env_cagetory_ls, \n",
    "     \"Approval date\": approval_date_ls,\n",
    "     \"PSD disclosed\": psd_disclosed_ls,\n",
    "     \"Project description\": project_des_ls,\n",
    "     \"Total project cost\": total_cost_ls,\n",
    "     \"Link\": no_des_link,\n",
    "     }\n",
    "df = pd.DataFrame(data = d)\n",
    "df.to_csv(\"../refine_output/refined_df_1.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: need human labor due to the various changes in web formats\n",
    "# fill in project description and cost by hand\n",
    "refine_Data = pd.read_csv(\"../refine_output/refine_df_2.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "refine_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_output_file = pre_output_file.dropna(subset=[\"Project description\"])\n",
    "web_scraping_result = pd.concat([refine_Data, pre_output_file])\n",
    "web_scraping_result[\"Project number\"] = web_scraping_result[\"Project number\"].astype(\"int32\")\n",
    "print(web_scraping_result.shape)\n",
    "web_scraping_result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read raw data\n",
    "raw_df = pd.read_csv(\"../input/downloadfile.csv\")\n",
    "print(raw_df.columns)\n",
    "print(raw_df.shape)\n",
    "raw_df.drop_duplicates(subset=[\" ProjectID\"], inplace=True)\n",
    "raw_df[\" ProjectID\"] = raw_df[\" ProjectID\"].astype(\"int32\")\n",
    "print(raw_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the web scraping results and raw data\n",
    "merge_df = web_scraping_result.merge(raw_df, how = \"right\", left_on=\"Project number\", right_on=\" ProjectID\").drop(columns=[\"Link\", \"Project number\"])\n",
    "merge_df[\" ProjectID\"] = merge_df[\" ProjectID\"].astype(\"int32\")\n",
    "print(merge_df.shape)\n",
    "merge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.to_csv(\"../refine_output/merge_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: need human labor due to the various changes in web formats\n",
    "# fill in location by hand\n",
    "merge_df = pd.read_csv(\"../refine_output/merge_df_2.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "print(merge_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Check if the location is the country level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df[\"country-level\"] = merge_df[[\"Location\",\" Country\"]].apply(lambda x: 1 if x.Location  == x[\" Country\"]  else 0, axis = 1)\n",
    "merge_df_ls = np.unique(merge_df[\"Location\"])\n",
    "merge_df_ls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Find the lat/long based on country name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_dict = dict()\n",
    "\n",
    "for i in merge_df_ls:\n",
    "    geolocator = Nominatim(user_agent=\"find\")\n",
    "    loc = geolocator.geocode(i)\n",
    "    try:\n",
    "        lat = loc.latitude\n",
    "        long = loc.longitude\n",
    "    except:\n",
    "        print(i)\n",
    "        lat = np.nan\n",
    "        long = np.nan\n",
    "    geo_dict[i]=(lat, long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the geolocation by hand\n",
    "geo_dict[\"Regional\"] = (np.nan, np.nan)\n",
    "geo_dict[\"FYR Macedonia\"] = (41.815338, 21.406864)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lat(x):\n",
    "    lat, long = geo_dict[x]\n",
    "    return lat\n",
    "\n",
    "def find_long(x):\n",
    "    lat, long = geo_dict[x]\n",
    "    return long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df[\"latitude\"]= merge_df[\"Location\"].apply(lambda x: find_lat(x))\n",
    "merge_df[\"longitude\"]= merge_df[\"Location\"].apply(lambda x: find_long(x))\n",
    "merge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the nan data\n",
    "merge_df[merge_df[\"latitude\"].isna()][\" Country\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.to_csv(\"../refine_output/final_Result.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
