{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install fastparquet &> /dev/null\n",
    "!pip install pyarrow &> /dev/null\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all folder names in S3 bucket\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket('atharva-54')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_name(path):\n",
    "    return path.split('/*.csv')\n",
    "\n",
    "# files = [obj.key for obj in my_bucket.objects.all()]\n",
    "parquet_files = [obj.key for obj in my_bucket.objects.filter(Prefix='PK')]\n",
    "parquet_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in my_bucket.objects.all():\n",
    "    print(obj.key, obj.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read xlsx file AttributeError: 'ElementTree' object has no attribute 'getiterator'\n",
    "theme_list = pd.read_excel('GKG theme list for R54.xlsx', engine=\"openpyxl\")\n",
    "#Remove rows with NaN values in all columns except 'GKG theme' and 'Human-readable'\n",
    "theme_list = theme_list.dropna(how='all', subset=['th_topic_2a_environment',\n",
    "       'th_topic_2b_regulation', 'topic_2a_2b_combined',\n",
    "       'th_topic_4_customer_orientation', 'th_topic_6_shareholder_affiliation',\n",
    "       'th_topic_8_hiring_employees', 'th_topic_9_natural_disaster',\n",
    "       'th_topic_11_litigation', 'th_topic_12_carbon_emissions',\n",
    "       'th_topic_13_labor_issues', 'topic_8_13_combined',\n",
    "       'th_topic_14_supply_chain_problems', 'th_topic_15_taxes']).reset_index(drop=True)\n",
    "#Drop Human-readable column\n",
    "theme_list = theme_list.drop(columns=['Human-readable'])\n",
    "#Make GKG theme uppercase\n",
    "theme_list['GKG theme'] = theme_list['GKG theme'].str.upper()\n",
    "#Drop duplicates in GKG theme column\n",
    "theme_list = theme_list.drop_duplicates(subset=['GKG theme'], keep='first').reset_index(drop=True)\n",
    "theme_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#If th_topic_2a_environment & th_topic_2b_regulation are both 1, then topic_2a_2b combined = 1\n",
    "theme_list['topic_2a_2b_combined'] = np.where((theme_list['th_topic_2a_environment'] == 1) & (theme_list['th_topic_2b_regulation'] == 1), 1, 0)\n",
    "\n",
    "#If th_topic_8_hiring_employees OR th_topic_13_labor_issues are both 1, then topic_8_13_combined = 1\n",
    "theme_list['topic_8_13_combined'] = np.where((theme_list['th_topic_8_hiring_employees'] == 1) | (theme_list['th_topic_13_labor_issues'] == 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#main\n",
    "main_columns = ['th_topic_2a_environment', 'th_topic_2b_regulation',\n",
    "       'topic_2a_2b_combined', 'th_topic_4_customer_orientation',\n",
    "       'th_topic_6_shareholder_affiliation', 'th_topic_8_hiring_employees',\n",
    "       'th_topic_9_natural_disaster', 'th_topic_11_litigation',\n",
    "       'th_topic_12_carbon_emissions', 'th_topic_13_labor_issues',\n",
    "       'topic_8_13_combined', 'th_topic_14_supply_chain_problems',\n",
    "       'th_topic_15_taxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imp_themes = list(theme_list['GKG theme'])\n",
    "#Capitalize all themes\n",
    "imp_themes = [x.upper() for x in imp_themes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a list of themes for each main column\n",
    "for i in main_columns:\n",
    "    globals()[i + '_themes'] = list(theme_list[theme_list[i] > 0]['GKG theme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# th_topic_2a_environment_themes is a list. Add \"mean_\" to each element in the list\n",
    "th_topic_2a_environment_themes_mean = ['mean_' + i for i in th_topic_2a_environment_themes]\n",
    "th_topic_2b_regulation_themes_mean = ['mean_' + i for i in th_topic_2b_regulation_themes]\n",
    "topic_2a_2b_combined_themes_mean = ['mean_' + i for i in topic_2a_2b_combined_themes]\n",
    "th_topic_4_customer_orientation_themes_mean = ['mean_' + i for i in th_topic_4_customer_orientation_themes]\n",
    "th_topic_6_shareholder_affiliation_themes_mean = ['mean_' + i for i in th_topic_6_shareholder_affiliation_themes]\n",
    "th_topic_8_hiring_employees_themes_mean = ['mean_' + i for i in th_topic_8_hiring_employees_themes]\n",
    "th_topic_9_natural_disaster_themes_mean = ['mean_' + i for i in th_topic_9_natural_disaster_themes]\n",
    "th_topic_11_litigation_themes_mean = ['mean_' + i for i in th_topic_11_litigation_themes]\n",
    "th_topic_12_carbon_emissions_themes_mean = ['mean_' + i for i in th_topic_12_carbon_emissions_themes]\n",
    "th_topic_13_labor_issues_themes_mean = ['mean_' + i for i in th_topic_13_labor_issues_themes]\n",
    "topic_8_13_combined_themes_mean = ['mean_' + i for i in topic_8_13_combined_themes]\n",
    "th_topic_14_supply_chain_problems_themes_mean = ['mean_' + i for i in th_topic_14_supply_chain_problems_themes]\n",
    "th_topic_15_taxes_themes_mean = ['mean_' + i for i in th_topic_15_taxes_themes]\n",
    "\n",
    "#For count\n",
    "th_topic_2a_environment_themes_count = ['count_' + i for i in th_topic_2a_environment_themes]\n",
    "th_topic_2b_regulation_themes_count = ['count_' + i for i in th_topic_2b_regulation_themes]\n",
    "topic_2a_2b_combined_themes_count = ['count_' + i for i in topic_2a_2b_combined_themes]\n",
    "th_topic_4_customer_orientation_themes_count = ['count_' + i for i in th_topic_4_customer_orientation_themes]\n",
    "th_topic_6_shareholder_affiliation_themes_count = ['count_' + i for i in th_topic_6_shareholder_affiliation_themes]\n",
    "th_topic_8_hiring_employees_themes_count = ['count_' + i for i in th_topic_8_hiring_employees_themes]\n",
    "th_topic_9_natural_disaster_themes_count = ['count_' + i for i in th_topic_9_natural_disaster_themes]\n",
    "th_topic_11_litigation_themes_count = ['count_' + i for i in th_topic_11_litigation_themes]\n",
    "th_topic_12_carbon_emissions_themes_count = ['count_' + i for i in th_topic_12_carbon_emissions_themes]\n",
    "th_topic_13_labor_issues_themes_count = ['count_' + i for i in th_topic_13_labor_issues_themes]\n",
    "topic_8_13_combined_themes_count = ['count_' + i for i in topic_8_13_combined_themes]\n",
    "th_topic_14_supply_chain_problems_themes_count = ['count_' + i for i in th_topic_14_supply_chain_problems_themes]\n",
    "th_topic_15_taxes_themes_count = ['count_' + i for i in th_topic_15_taxes_themes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read PRIO GRID spine\n",
    "spine = pd.read_csv('PRIO GRID spine.csv')\n",
    "\n",
    "def lat_long_to_PG_lat_long(x):\n",
    "    list_lat_long = []\n",
    "    for i in range(len(x)):\n",
    "        if x[i] >= 0:\n",
    "            pg_val = int(x[i]) + 0.25 if x[i] % 1 >= 0 and x[i] % 1 <= 0.5 else int(x[i]) + 0.75\n",
    "        else:\n",
    "            pg_val = int(x[i]) - 0.75 if x[i] % 1 >= 0 and x[i] % 1 <= 0.5 else int(x[i]) - 0.25\n",
    "        list_lat_long.append(pg_val)\n",
    "    return list_lat_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countries = ['PK', 'CO', 'WI', 'NS', 'LI', 'CA', 'NC', 'WE', 'BG', 'GR', 'FI', 'QA', 'VM', 'LY', 'BR', 'MY', 'EU', 'BU', 'FK', 'RO', 'KN', 'AE', 'RS', 'BA', 'VI', 'UG', 'PO', 'TK', 'BL', 'RE', 'DA', 'EI', 'AU', 'TN', 'KT', 'MG', 'BC', 'MA', 'CG', 'AY', 'SH', 'SP', 'NF', 'AS', 'VC', 'GL', 'RN', 'JN', 'FJ', 'MD', 'UP', 'BO', 'IN', 'HK', 'MV', 'GI', 'MU', 'MP', 'GP', 'GT', 'SZ', 'LS', 'AO', 'RM', 'TH', 'JM', 'FP', 'MT', 'PC', 'PP', 'SM', 'AJ', 'LT', 'EZ', 'KU', 'AC', 'RI', 'CH', 'BK', 'CW', 'EG', 'DJ', 'GM', 'OS', 'TU', 'CT', 'BN', 'PF', 'NH', 'MZ', 'ID', 'KZ', 'FO', 'BB', 'SV', 'CU', 'OC', 'NG', 'AQ', 'ET', 'TW', 'LO', 'GH', 'IM', 'GK', 'FG', 'LQ', 'CJ', 'NP', 'CI', 'TL', 'MN', 'GZ', 'HR', 'SU', 'HU', 'BD', 'LG', 'CY', 'CN', 'OD', 'PE', 'UV', 'SC', 'GJ', 'MK', 'GO', 'BV', 'CQ', 'BP', 'TE', 'EK', 'AR', 'ST', 'IC', 'SE', 'DR', 'BF', 'IS', 'TI', 'SL', 'MI', 'PG', 'UK', 'NZ', 'WA', 'MC', 'HO', 'LE', 'IO', 'DQ', 'AM', 'CD', 'NE', 'UY', 'JE', 'ER', 'BE', 'KR', 'WS', 'SY', 'IZ', 'CS', 'AG', 'NL', 'ML', 'GQ', 'CB', 'SG', 'IP', 'MB', 'IT', 'TX', 'LU', 'PA', 'LH', 'RP', 'AF', 'NT', 'PU', 'CF', 'WZ', 'EC', 'NI', 'TV', 'BY', 'NU', 'FR', 'NO', 'BX', 'NR', 'IV', 'TP', 'MH', 'KS', 'BT', 'JO', 'AV', 'TD', 'TZ', 'JA', 'KV', 'SA', 'YI', 'SF', 'GV', 'YM', 'SO', 'HQ', 'HA', 'JQ', 'TS', 'VT', 'SW', 'ZI', 'SB', 'VE', 'TT', 'GB', 'KE', 'GG', 'FM', 'CV', 'CM', 'KQ', 'MO', 'SN', 'ZA', 'TO', 'WQ', 'PL', 'CE', 'RW', 'KG', 'SI', 'MR', 'MX', 'EN', 'ES', 'RB', 'MF', 'GA', 'LA', 'AL', 'US', 'MQ', 'BQ', 'UZ', 'PM', 'FQ', 'MJ', 'DO', 'GY', 'BH', 'IR', 'BM', 'PS', 'VQ', 'AN', 'RQ', 'CR', 'JU', 'WF', 'CK', 'UC', 'FS', 'HM', 'TB', 'NN', 'SX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Gather all split data into one dataframe\n",
    "\n",
    "def gather(countries):\n",
    "    #Make an empty dataframe to store the appended data\n",
    "    for country in countries:\n",
    "        parquet_files = [obj.key for obj in my_bucket.objects.filter(Prefix=country)]\n",
    "        print(country)\n",
    "    \n",
    "        df_main = pd.DataFrame()\n",
    "        for filer in parquet_files:\n",
    "            print(filer)\n",
    "            #Read parquet file and specify columns to read\n",
    "            df = pd.read_parquet('s3://atharva-54/'+filer, columns=['date', 'tone', 'themes', 'locations'])\n",
    "\n",
    "            #Keep date from timestamp\n",
    "            df['date'] = df['date'].dt.date\n",
    "\n",
    "            #Keep \"tone\" from \"tone\" column which is a dictionary\n",
    "            df['tone'] = df['tone'].apply(lambda x: x['tone'])\n",
    "\n",
    "            #Get \"theme\" from each dictionary and make a list of themes\n",
    "            df['themes'] = df['themes'].apply(lambda x: [i['theme'] for i in x] if x is not None else None)\n",
    "\n",
    "            #Explode themes column\n",
    "            df = df.explode('themes').reset_index(drop=True)\n",
    "\n",
    "            #Keep only rows with themes in imp_themes\n",
    "            df = df[df['themes'].isin(imp_themes)].reset_index(drop=True)\n",
    "\n",
    "            #Get \"location\" from each dictionary and make a list of locations. If there are duplicates in the list, keep only one\n",
    "            df['locations'] = df['locations'].apply(lambda x: [[i['location_latitude'], i['location_longitude']] for i in x] if x is not None else None)\n",
    "\n",
    "            #Explode locations column\n",
    "            df = df.explode('locations').reset_index(drop=True)\n",
    "\n",
    "            #From location, keep first value as latitude and second value as longitude\n",
    "            df['latitude'] = df['locations'].apply(lambda x: x[0])\n",
    "            df['longitude'] = df['locations'].apply(lambda x: x[1])\n",
    "\n",
    "            #Drop locations column\n",
    "            df = df.drop(columns=['locations'])\n",
    "\n",
    "            #Remove duplicate rows\n",
    "            df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "            #Create new columns for PG latitude and longitude\n",
    "            df['lat'] = np.nan\n",
    "            df['lon'] = np.nan\n",
    "            \n",
    "            #drop row with latitude column = ''\n",
    "            df = df[df['latitude'] != ''].reset_index(drop=True)\n",
    "            df = df[df['longitude'] != ''].reset_index(drop=True)\n",
    "\n",
    "            #Make latitude and longitude columns into float\n",
    "            df['latitude'] = df['latitude'].astype(float)\n",
    "            df['longitude'] = df['longitude'].astype(float)\n",
    "\n",
    "            #Convert latitude and longitude to PG latitude and longitude\n",
    "            df['lat'] = lat_long_to_PG_lat_long(df[\"latitude\"])\n",
    "            df['lon'] = lat_long_to_PG_lat_long(df[\"longitude\"])\n",
    "\n",
    "            #Drop latitude and longitude columns\n",
    "            df = df.drop(columns=['latitude', 'longitude'])\n",
    "\n",
    "            #Merge df with spine\n",
    "            df = df.merge(spine, how='left', left_on=['lat', 'lon'], right_on=['lat', 'lon']).reset_index(drop=True)\n",
    "\n",
    "            #Drop lat and lon columns\n",
    "            df = df.drop(columns=['lat', 'lon'])\n",
    "\n",
    "            #Drop rows with NaN values in gid column\n",
    "            df = df.dropna(subset=['gid']).reset_index(drop=True)\n",
    "\n",
    "            #Make GID column into int\n",
    "            df['gid'] = df['gid'].astype(int)\n",
    "\n",
    "            #Convert tone column to float\n",
    "            df['tone'] = df['tone'].astype(float)\n",
    "\n",
    "            #Convert date column to datetime\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "            # Create a new DataFrame to store the appended themes\n",
    "            new_df = pd.DataFrame({'themes': imp_themes})\n",
    "            # Fill the date column with the previous value\n",
    "            new_df['date'] = df['date'].ffill()[0]\n",
    "            # Fill the tone and gid columns with zeros\n",
    "            new_df['tone'] = 0\n",
    "            new_df['gid'] = 0\n",
    "\n",
    "            # Append the new DataFrame to the original DataFrame\n",
    "            df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "            # Fill all remaining NaN values with np.NaN\n",
    "            df = df.fillna(np.NaN)\n",
    "\n",
    "            #Less Efficient: Append imp_themes to df under column 'themes' and fill other columns with np.NaN\n",
    "            # for i in imp_themes:\n",
    "            #     df = df.append({'themes':i}, ignore_index=True)\n",
    "            #     #Date column to be ffilled\n",
    "            #     df['date'] = df['date'].ffill()\n",
    "            #     #Fill tone & gid column with 0\n",
    "            #     df['tone'] = df['tone'].fillna(0)\n",
    "            #     df['gid'] = df['gid'].fillna(0)\n",
    "            # df = df.fillna(np.NaN)\n",
    "\n",
    "            #Pivot table to get mean tone and count of themes per gid and date\n",
    "            df = df.pivot_table(index=['gid', 'date'], columns='themes', values='tone', aggfunc=['mean', 'count']).reset_index().sort_values(by=['gid', 'date'])\n",
    "\n",
    "            #make only one level of column names, add upper level name to lower level name\n",
    "            df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
    "\n",
    "            #Drop gid == 0\n",
    "            df = df[df['gid'] != 0].reset_index(drop=True)\n",
    "\n",
    "            #Sort by gid\n",
    "            df = df.sort_values(by=['gid']).reset_index(drop=True)\n",
    "\n",
    "            #Replace whitespace in column names with underscore\n",
    "            df.columns = df.columns.str.replace(' ', '_')   \n",
    "\n",
    "            #Take column wise mean of column names present in th_topic_2a_environment_themes for each row and store in column \"average_th_topic_2a_environment\"\n",
    "            df['average_th_topic_2a_environment'] = df[th_topic_2a_environment_themes_mean].mean(axis=1)\n",
    "            df['average_th_topic_2b_regulation'] = df[th_topic_2b_regulation_themes_mean].mean(axis=1)\n",
    "            df['average_topic_2a_2b_combined'] = df[topic_2a_2b_combined_themes_mean].mean(axis=1)\n",
    "            df['average_th_topic_4_customer_orientation'] = df[th_topic_4_customer_orientation_themes_mean].mean(axis=1)\n",
    "            df['average_th_topic_6_shareholder_affiliation'] = df[th_topic_6_shareholder_affiliation_themes_mean].mean(axis=1)\n",
    "            df['average_th_topic_8_hiring_employees'] = df[th_topic_8_hiring_employees_themes_mean].mean(axis=1)\n",
    "            df['average_th_topic_9_natural_disaster'] = df[th_topic_9_natural_disaster_themes_mean].mean(axis=1)\n",
    "            df['average_th_topic_11_litigation'] = df[th_topic_11_litigation_themes_mean].mean(axis=1)\n",
    "            df['average_th_topic_12_carbon_emissions'] = df[th_topic_12_carbon_emissions_themes_mean].mean(axis=1)\n",
    "            df['average_th_topic_13_labor_issues'] = df[th_topic_13_labor_issues_themes_mean].mean(axis=1)\n",
    "            df['average_topic_8_13_combined'] = df[topic_8_13_combined_themes_mean].mean(axis=1)\n",
    "            df['average_th_topic_14_supply_chain_problems'] = df[th_topic_14_supply_chain_problems_themes_mean].mean(axis=1)\n",
    "            df['average_th_topic_15_taxes'] = df[th_topic_15_taxes_themes_mean].mean(axis=1)\n",
    "\n",
    "            #For count\n",
    "            df['cnt_th_topic_2a_environment'] = df[th_topic_2a_environment_themes_count].sum(axis=1)\n",
    "            df['cnt_th_topic_2b_regulation'] = df[th_topic_2b_regulation_themes_count].sum(axis=1)\n",
    "            df['cnt_topic_2a_2b_combined'] = df[topic_2a_2b_combined_themes_count].sum(axis=1)\n",
    "            df['cnt_th_topic_4_customer_orientation'] = df[th_topic_4_customer_orientation_themes_count].sum(axis=1)\n",
    "            df['cnt_th_topic_6_shareholder_affiliation'] = df[th_topic_6_shareholder_affiliation_themes_count].sum(axis=1)\n",
    "            df['cnt_th_topic_8_hiring_employees'] = df[th_topic_8_hiring_employees_themes_count].sum(axis=1)\n",
    "            df['cnt_th_topic_9_natural_disaster'] = df[th_topic_9_natural_disaster_themes_count].sum(axis=1)\n",
    "            df['cnt_th_topic_11_litigation'] = df[th_topic_11_litigation_themes_count].sum(axis=1)\n",
    "            df['cnt_th_topic_12_carbon_emissions'] = df[th_topic_12_carbon_emissions_themes_count].sum(axis=1)\n",
    "            df['cnt_th_topic_13_labor_issues'] = df[th_topic_13_labor_issues_themes_count].sum(axis=1)\n",
    "            df['cnt_topic_8_13_combined'] = df[topic_8_13_combined_themes_count].sum(axis=1)\n",
    "            df['cnt_th_topic_14_supply_chain_problems'] = df[th_topic_14_supply_chain_problems_themes_count].sum(axis=1)\n",
    "            df['cnt_th_topic_15_taxes'] = df[th_topic_15_taxes_themes_count].sum(axis=1)\n",
    "\n",
    "            #Delete all columns not beginning with 'average' or 'count'. Keep gid & date\n",
    "            df = df[[col for col in df.columns if col.startswith('average') or col.startswith('cnt') or col.startswith('gid') or col.startswith('date')]]\n",
    "\n",
    "            #Concatenate df_main with df\n",
    "            df_main = pd.concat([df_main, df], ignore_index=True) \n",
    "        #Perform final groupby on df_main\n",
    "        df_main_annual = pd.concat([df_main.groupby(['gid', df_main['date'].dt.year]).agg({col:'mean' for col in df_main.columns if col.startswith('average')}).reset_index(), df_main.groupby(['gid', df_main['date'].dt.year]).agg({col:'sum' for col in df_main.columns if col.startswith('cnt')}).reset_index(drop=True)], axis=1)\n",
    "        df_main_quarterly = pd.concat([df_main.groupby(['gid', pd.PeriodIndex(df_main['date'], freq='Q')]).agg({col:'mean' for col in df_main.columns if col.startswith('average')}).reset_index(), df_main.groupby(['gid', pd.PeriodIndex(df_main['date'], freq='Q')]).agg({col:'sum' for col in df_main.columns if col.startswith('cnt')}).reset_index(drop=True)], axis=1)\n",
    "\n",
    "        #take the mean of columns beginning with 'average', and store in column 'average_tone', row-wise\n",
    "        df_main_annual['average_tone'] = df_main_annual[[col for col in df_main_annual.columns if col.startswith('average')]].mean(axis=1)\n",
    "\n",
    "        #take the sum of columns beginning with 'cnt', and store in column 'cnt_tone', row-wise\n",
    "        df_main_annual['total_articles'] = df_main_annual[[col for col in df_main_annual.columns if col.startswith('cnt')]].sum(axis=1)\n",
    "\n",
    "        #take the mean of columns beginning with 'average', and store in column 'average_tone', row-wise\n",
    "        df_main_quarterly['average_tone'] = df_main_quarterly[[col for col in df_main_quarterly.columns if col.startswith('average')]].mean(axis=1)\n",
    "\n",
    "        #take the sum of columns beginning with 'cnt', and store in column 'cnt_tone', row-wise\n",
    "        df_main_quarterly['total_articles'] = df_main_quarterly[[col for col in df_main_quarterly.columns if col.startswith('cnt')]].sum(axis=1)\n",
    "\n",
    "        return df_main_annual, df_main_quarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gather(countries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
